<!DOCTYPE html>
<html>
  <head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type">
    <meta name="generator" content="HTML Tidy for HTML5 (experimental) for Linux https://github.com/w3c/tidy-html5/tree/c63cc39">
    <meta content="width=device-width,initial-scale=1" name="viewport">
    <title>Data on the Web Best Practices</title>
    <script class="remove" src="http://www.w3.org/Tools/respec/respec-w3c-common"></script>
    <script class="remove" src="bpconfig.js"></script>
    <script src="cross-dom.js"></script>
    <script>
      window.onload=init;
    </script>
    <style type="text/css">

#bp-summary ul {
  list-style-type: none;
  padding-left: 0;
  line-height:1.6em;
  background-color: #FCFAEE;
}

@media screen and (min-width: 600px) {
  #bp-summary ul {
    column-count:2;
    -moz-column-count:2;
    -webkit-column-count:2;
    column-gap: 1em;
  }
}

.practice, #tempPractice {
  padding-left: 1em;
  background-color: #FCFAEE;
  border: thin solid black;
}

#tempPractice .tempPracticelab {
  background-color:#dfffff;
  position: relative;
  top: -1.5em;
  font-weight:bold;
}

.practice p.practicedesc, #tempPractice p.tempPracticedesc {
  font-style:italic;
  border-bottom: thin solid black;
  position:relative;
  top:-1.5em;
  margin: 0 2em -1em 1em;
}

.subhead{
  font-weight:bold;
}
.benefits .stamp {
  height: 52px;
  width: 52px;
  margin-right: 4px;
  margin-bottom: 4px;
}

.practice dl dt #tempPractice dl dt{
  font-weight:normal;
}

figure {
  text-align:center;
}

figure#contextDiagram {
  width:60%;
  margin:0 auto;
}

figure figcaption {
  text-align:center;
  font-style:italic;
}

table#uripatternstable {
  border-collapse: collapse;
  caption-side:bottom;
}
table#uripatternstable th, table#uripatternstable td {
  border: 1px solid black;
  padding:0.3em;
}

table#uripatternstable caption {
  margin:0.5em;
  font-style:italic;
}

.stmt
{       
    padding: 3pt}

.stmt1
{
    column-count:2;
    -moz-column-count:2;
    -webkit-column-count:2;
    column-gap: 1em;
    background-color: #FCFAEE;
 }

  .expand{
    display:block;
    cursor: pointer;
  }
  .expand:hover {
    color: #3D3D3D;
  }
  .expand:before {
    font-weight: bold;
    content: "\25C6  Example (click to expand or collapse)";
  }
  .expand + input{
    display:none;
  }
  .expand + input + *{
    display:none;
  }
  .expand + input:checked + *{
    display:block;
  }

  </style>
  </head>
  <body>
    <section id="abstract">
      <p>This document provides best practices related to the publication and
        usage of data on the Web designed to help support a self-sustaining
        ecosystem. Data should be discoverable and understandable by humans and
        machines. Where data is used in some way, whether by the originator of
        the data or by an external party, such usage should also be discoverable
        and the efforts of the data publisher recognized. In short, following
        these best practices will facilitate interaction between publishers and
        consumers.</p>
    </section>
    <section id="sotd">
      <p>This version of the document shows its expected scope and future
        direction. A template is used to show the "what", "why" and "how" of
        each best practice. Comments are sought on the usefulness of this
        approach and the expected scope of the final document.</p>
    </section>
    <section id="intro" class="informative">
      <h2>Introduction</h2>
      <p>The best practices described below have been developed to encourage and
        enable the continued expansion of the Web as a medium for the exchange
        of data. The growth of open data by governments across the world
        [[OKFN-INDEX]], the increasing publication of research data encouraged
        by organizations like the Research Data Alliance [[RDA]], the harvesting
        and analysis of social media, crowd-sourcing of information, the
        provision of important cultural heritage collections such as at the
        Biblioth√®que nationale de France [[BNF]] and the sustained growth in the
        Linked Open Data Cloud [[LODC]], provide some examples of this
        phenomenon.</p>
      <p>In broad terms, data publishers aim to share data either openly or with
        controlled access. Data consumers (who may also be producers themselves)
        want to be able to find and use data, especially if it is accurate,
        regularly updated and guaranteed to be available at all times. This
        creates a fundamental need for a common understanding between data
        publishers and data consumers. Without this agreement, data publishers'
        efforts may be incompatible with data consumers' desires.</p>
      <p>Publishing data on the Web creates new challenges, such as how to
        represent, describe and make data available in a way that it will be
        easy to find and to understand. In this context, it becomes crucial to
        provide guidance to publishers that will improve consistency in the way
        data is managed, thus promoting the re-use of data and also to foster
        trust in the data among developers, whatever technology they choose to
        use, increasing the potential for genuine innovation.</p>
      <p>This document sets out a series of best practices that will help
        publishers and consumers face the new challenges and opportunities posed
        by data on the Web. </p>
      <p>Best practices cover different aspects related to data publishing and
        consumption, like data formats, data access, data identifiers and
        metadata. In order to delimit the scope and elicit the required features
        for Data on the Web Best Practices, the <abbr title="Data on the Web Best Practices">DWBP</abbr>
        working group compiled a set of use cases [[UCR]] that represent
        scenarios of how data is commonly published on the Web and how it is
        used. The set of requirements derived from these use cases were used to
        guide the development of the best practices.</p>
      <p>The Best Practices proposed in this document are intended to serve a
        more general purpose than the practices suggested in Best Practices for
        Publishing Linked Data [[LD-BP]] since it is domain-independent and
        whilst it recommends the use of Linked Data, it also promotes best
        practices for data on the web in formats such as [[CSV]] and [[JSON]].
        The Best Practices related to the use of vocabularies incorporate
        practices that stem from Best Practices for Publishing Linked Data where
        appropriate.</p>
    </section>
    <!--     <section id="conformance"> </section> -->
    <section id="audience" class="informative">
      <h2>Audience</h2>
      <p>This document provides best practices to those who publish data on the
        Web. The best practices are designed to meet the needs of information
        management staff, developers, and wider groups such as scientists
        interested in sharing and re-using research data on the Web. While data
        publishers are our primary audience, we encourage all those engaged in
        related activities to become familiar with it. Every attempt has been
        made to make the document as readable and usable as possible while still
        retaining the accuracy and clarity needed in a technical specification.</p>
      <p>Readers of this document are expected to be familiar with some
        fundamental concepts of the architecture of the Web [[WEBARCH]], such as
        resources and URIs, as well as a number of data formats. The normative
        element of each best practice is the <em>intended outcome</em>.
        Possible implementations are suggested and, where appropriate, these
        recommend the use of a particular technology such as <abbr title="Comma Separataed Variables">CSV</abbr>,
        JSON or RDF. A basic knowledge of vocabularies and data models would be
        helpful to better understand some aspects of this document. </p>
    </section>
    <section id="scope" class="informative">
      <h2>Scope</h2>
      <p>This document is concerned solely with best practices that:</p>
      <ul>
        <li>are specifically relevant to data published on the Web;</li>
        <li>encourage publication or re-use of data on the Web;</li>
        <li>can be tested by machines, humans or a combination of the two.</li>
      </ul>
      <p>As noted above, whether a best practice has or has not been followed
        should be judged against the <em>intended outcome</em>, not the <em>possible
          approach to implementation</em> which is offered as guidance. A best
        practice is always subject to improvement as we learn and evolve the Web
        together.</p>
    </section>
    <section id="context" class="informative">
      <h2>Context</h2>
      <p> In general, the Best Practices proposed for publication and usage of
        Data on the Web refer to datasets and distributions. For this document,
        as defined by [[DCAT]], "a dataset is a collection of data, available
        for access or download in one or more formats". By data, "we mean known
        facts that can be recorded and that have implicit meaning" [[Navathe]].
        In ordet to meet the requirements of several data consumers datasets
        should be available in more than one distribution. Again quoting the
        [[DCAT]] specification, a distribution "Represents a specific available
        form of a dataset. Each dataset might be available in different forms,
        these forms might represent different formats of the dataset or
        different endpoints. Examples of distributions include a downloadable
        CSV file, an API or an RSS feed".</p>
      <p> In our context, two importante aspects should be considered when
        publishing data on the Web: the sharing of data in a large scale and the
        use of the Web as a platform for data publication and sharing. Large
        scale data sharing allows datasets to be used for several groups of data
        consumers, which may have distinct requirements and expectations about
        the data. Given this heterogeneity and the fact that data publishers and
        data consumers may be unknown to each other, it is necessary to offer
        some additional information to help understanding and manipulating the
        data. Such information may also contribute to leverage dataset
        trustworthy and reuse. Additional information that should be published
        as part of the dataset includes but is not limited to: structural
        metadata, descriptive metadata, access information, data quality
        information, provenance information, license information and usage
        information. </p>
      <p> The second aspect, the use of the Web as a platform for data
        publication and sharing, concerns the architectural bases of the Web as
        discussed in [[WEBARCH]]. The DWBP document is mainly interested on the
        Identification principle that says that URIs should be used to identify
        resources. In our context, a resource may be a whole dataset or a
        specific item of given dataset. All resources should be published with
        stable URIs, so that they can be referenced. At the very least, give
        them unique and stable URIs, if you don't want to make them directly
        accessible. Besides it should be possible to create links between
        resources. A link may be defined as a relationship between two resources
        when one resource refers to the other resource by means of a URI. </p>
      <p>The following diagram illustrates the dataset composition (data values
        and metadata) together with other components related to the dataset
        publication and usage. Data values correspond to the data itself and may
        be available in one or more distributions, which should be defined by
        the publisher considering data consumer's expectations. The Metadata
        component corresponds to the additional information that describes the
        dataset and dataset distributions, helping the manipulation and the
        reuse of the data.&nbsp; In order to allow an easy access to the dataset
        and its corresponding distributions multiple Dataset Access mechanisms
        should be available such as APIs and bulk download. Finally, to promote
        the interoperability among datasets it is important to adopt Data
        Vocabularies and Standards. </p>
      <img src="images/context.jpg" alt="Our Context" height="440" width="640">
    </section>
    <section id="challenges" class="informative">
      <h2>Data on the Web Challenges</h2>
      <p>The openness and flexibility of the Web creates new challenges for data
        publishers and data consumers. In contrast to conventional databases,
        for example, where there is a single data model to represent the data
        and a database management system (DBMS) to control data access, data on
        the Web allows for the existence of multiple ways to represent and to
        access data. Furthermore, publishers and consumers may be unknown to
        each other and be part of entirely disparate communities with different
        norms and in-built assumptions so that it becomes essential to provide
        information about data structure, quality, provenance and any terms of
        use. The following diagram summarizes some of the main challenges faced
        when publishing or consuming data on the Web. These challenges were
        identified from the <abbr title="Data on the Web Best Practices">DWBP</abbr>
        Use Cases and Requirements [[UCR]] and are described by one or more
        questions. As presented in the diagram, each one of these challenges is
        addressed by one or more best practices.</p>
      <embed type="image/svg+xml" src="challenges.svg" style="width:100%" id="challengesSVG">
      <p> Each one of these challenges originated one or more requirements as
        documented in <a href="http://www.w3.org/TR/dwbp-ucr/#requirements">the
          use-cases document</a>. The development of Data on the Web Best
        Practices were guided by these requirements, in such a way that each
        best practice should have at least one of these requirements as an
        evidence of its relevance.</p>
    </section>
    <section id="bp-benefits" class="informative">
      <h2>Best Practices Benefits</h2>
      <p>In order to encourage data publishers to adopt the DWBP, the list below
        describes the main benefits of applying the proposed BP. Each benefit
        represents an improvement in the way how datasets are available on the
        Web.</p>
      <ul>
        <li> Comprehension: humans will have a better understanding about the
          data structure, the data meaning, the metadata and the nature of the
          dataset. </li>
        <li> Processibility: machines will be able to automatically process and
          manipulate the data within a dataset.</li>
        <li> Discoverability machines will be able to automatically discover a
          dataset or data within a dataset.</li>
        <li> Reuse: the chances of dataset reuse by different groups of data
          consumers will increase.</li>
        <li> Trustworthy: the confidence of consumers on the dataset will
          improve.</li>
        <li> Linkability: it will be possible to create links between data
          resources (datasets and data items).</li>
        <li> Accessibility: humans and machines will be able to access the data
          up to date in a variety of forms.</li>
        <li> Interoperability: it will be easier to reach consensus among data
          publishers and consumers.</li>
      </ul>
      The figure below shows the benefits that data publishers will gain with
      the adoption of each one of the proposed best practices. <img src="images/benefits.jpg"

        alt="BP Benefits" height="340" width="840"> </section>
    <section id="bp-template">
      <h2>Best Practices Template</h2>
      <p>This section presents the template used to describe Data on the Web
        Best Practices.</p>
      <div id="tempPractice">
        <p><span id="template" class="tempPracticelab">Best Practice Template</span></p>
        <p class="tempPracticedesc">Short description of the BP</p>
        <section class="axioms">
          <p class="subhead">Why</p>
          <p>This section answers two crucial questions:</p>
          <ul>
            <li>Why this is unique to publishing or re-using data on the Web? </li>
            <li>How does this encourages publication or re-use of data on the
              Web? </li>
          </ul>
        </section>
        <section class="description">A full text description of the problem
          addressed by the best practice may also be provided. It can be any
          length but is likely to be no more than a few sentences. </section>
        <section class="outcome">
          <p class="subhead">Intended Outcome</p>
          <p>What it should be possible to do when a data publisher follows the
            best practice. </p>
        </section>
        <section class="how">
          <p class="subhead">Possible Approach to Implementation</p>
          <p>A description of a possible implementation strategy is provided.
            This represents the best advice available at the time of writing but
            specific circumstances and future developments may mean that
            alternative implementation methods are more appropriate to achieve
            the intended outcome.</p>
        </section>
        <section class="test">
          <p class="subhead">How to Test</p>
          <p>Information on how to test the BP has been met. This might or might
            not be machine testable.</p>
        </section>
        <section class="ucr">
          <p class="subhead">Evidence</p>
          <p>Information about the relevance of the BP. It is described by one
            or more relevant requirements as documented in the <a href="http://www.w3.org/TR/dwbp-ucr/">Data
              on the Web Best Practices Use Cases &amp; Requirements document</a></p>
        </section>
        <section class="benefits">
          <p class="subhead">Benefits</p>
          <!-- Trustworthy -->
          <svg xmlns="http://www.w3.org/2000/svg" class="stamp">
            <g>
              <circle cx="26" cy="26" r="25" fill="#BF5B0E" stroke-width="2" stroke="#BF5B0E"

                style="fill-opacity:0.4;"></circle>
              <text x="14" y="41" font-family="Arial" font-size="40" fill="#BF5B0E">T</text>
            </g> </svg>
          <!-- Reuse -->
          <svg xmlns="http://www.w3.org/2000/svg" class="stamp">
            <g>
              <circle cx="26" cy="26" r="25" fill="#387F05" stroke-width="2" stroke="#387F05"

                style="fill-opacity:0.4;"></circle>
              <text x="12" y="41" font-family="Arial" font-size="40" fill="#387F05">R</text>
            </g> </svg>
          <!-- Linkability -->
          <svg xmlns="http://www.w3.org/2000/svg" class="stamp">
            <g>
              <circle cx="26" cy="26" r="25" fill="#50637F" stroke-width="2" stroke="#50637F"

                style="fill-opacity:0.4;"></circle>
              <text x="14" y="41" font-family="Arial" font-size="40" fill="#50637F">L</text>
            </g> </svg>
          <!-- Processability -->
          <svg xmlns="http://www.w3.org/2000/svg" class="stamp">
            <g>
              <circle cx="26" cy="26" r="25" fill="#6E46AD" stroke-width="2" stroke="#6E46AD"

                style="fill-opacity:0.4;"></circle>
              <text x="14" y="41" font-family="Arial" font-size="40" fill="#6E46AD">P</text>
            </g> </svg>
          <!-- Interoperability -->
          <svg xmlns="http://www.w3.org/2000/svg" class="stamp">
            <g>
              <circle cx="26" cy="26" r="25" fill="#E0B200" stroke-width="2" stroke="#E0B200"

                style="fill-opacity:0.4;"></circle>
              <text x="20" y="41" font-family="Arial" font-size="40" fill="#E0B200">I</text>
            </g> </svg>
          <!-- Comprehension -->
          <svg xmlns="http://www.w3.org/2000/svg" class="stamp">
            <g>
              <circle cx="26" cy="26" r="25" fill="#404040" stroke-width="2" stroke="#404040"

                style="fill-opacity:0.4;"></circle>
              <text x="13" y="41" font-family="Arial" font-size="40" fill="#404040">C</text>
            </g> </svg>
          <!-- Discoverability -->
          <svg xmlns="http://www.w3.org/2000/svg" class="stamp">
            <g>
              <circle cx="26" cy="26" r="25" fill="#7F5C46" stroke-width="2" stroke="#7F5C46"

                style="fill-opacity:0.4;"></circle>
              <text x="13" y="41" font-family="Arial" font-size="40" fill="#7F5C46">D</text>
            </g> </svg> </section>
      </div>
      <!--       <div class="issue">
        <p>Which section of a BP is normative, and whether the use of RFC2119          keywords is appropriate, is <a href="http://www.w3.org/2013/dwbp/track/issues/146">Issue-146</a></p>      </div> -->
    </section>
    <section id="bp-summary"> </section>
    <section id="bestPractices">
      <h2>The Best Practices</h2>
      <p>This section contains the best practices to be used by data publishers
        in order to help them and data consumers to overcome the different
        challenges faced when publishing and consuming data on the Web. One or
        more best practices were proposed for each one of the previously
        described challenges. Each BP is related to one or more requirements
        from the <a href="http://www.w3.org/TR/dwbp-ucr/">Data on the Web Best
          Practices Use Cases &amp; Requirements document.</a></p>
      <section id="basicExample">
        <h3>Example</h3>
        <p> This example serves as a basis for elaboration that will be
          described in subsequent sections. It helps to illustrate how best
          practices may be applied. </p>
        <div class="example">
          <p> John works for the Transport Agency of MyCity and he is in charge
            of the publication of data on the Web about bus timetables as well
            as real time data about the traffic of the city. John decides to
            create two datasets: one for the bus timetables and other one for
            the real time traffic data. </p>
          <p> Some requirements that should be addressed:</p>
          <ul>
            <li> The dataset for bus timetables must be available in two
              languages: english and portuguese; </li>
            <li> Both datasets must be available in csv and json-ld formats; </li>
          </ul>
        </div>
        When necessary RDF examples will be used to show the result of the
        application of some best practices. RDF examples in this document are
        written in Turtle syntax [[TURTLE]] and [[JSON-LD]]. </section>
      <div class="note">
        <p> In this current version, examples are presented just in Turtle
          syntax. </p>
      </div>
      <section id="metadata">
        <h3>Metadata</h3>
        <p>The Web is an open information space, where the absence of a specific
          context, such a company's internal information system, means that the
          provision of metadata is a fundamental requirement. Data will not be
          discoverable or reusable by anyone other than the publisher if
          insufficient metadata is provided. Metadata provides additional
          information that helps data consumers better understand the meaning of
          data, its structure, and to clarify other issues, such as rights and
          license terms, the organization that generated the data, data quality,
          data access methods and the update schedule of datasets. <br>
        </p>
        Metadata can be used to help tasks such as dataset discovery and re-use,
        and can be assigned considering different levels of granularity from a
        single property of a resource to a whole dataset, or all datasets from a
        specific organization. <br>
        <p>Metadata can be of different types. These types can be classified in
          different taxonomies, with different grouping criteria. For example, a
          specific taxonomy could define three metadata types according to
          descriptive, structural and administrative features. Descriptive
          metadata serves to identify a dataset, structural metadata serves to
          understand the structure in which the dataset is distributed and
          administrative metadata serves to provide information about the
          version, update schedule etc. A different taxonomy could define
          metadata types with a scheme according to tasks where metadata are
          used, for example, discovery and re-use. </p>
        <p> </p>
        <!-- begin of Provide Metadata BP -->
        <div class="practice">
          <p><span id="ProvideMetadata" class="practicelab">Provide metadata</span></p>
          <p class="practicedesc">Metadata must be provided for both human users
            and computer applications</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Providing metadata is a fundamental requirement when publishing
              data on the Web because data publishers and data consumers may be
              unkown to each other. Then, it is essential to provide information
              that helps data consumers, i.e., human users and computer
              applications, to understand the data as well as other important
              aspects that describes a dataset.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p> It must be possible for humans to understand the metadata, which
              makes it <span style="font-style: italic;">human readable
                metadata</span>.</p>
            <p>It should be possible for computer applications, notably user
              agents, to process the metadata, which makes it <span style="font-style: italic;">machine
                readable metadata</span>.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            Possible approaches to provide <em>human readable metadata:</em><br>
            <ul>
              <li>to provide metadata as part of an HTML Web page</li>
              <li>to provide metadata as a separate text file</li>
            </ul>
            Possible approaches to provide <em>machine readable metadata:</em><br>
            <ul>
              <li> machine readable metadata may be provided in a serialization
                format such as Turtle and JSON, or it can be embedded in the
                HTML page using [[HTML-RDFA]] or [[JSON-LD]]. If multiple
                formats are published separately, they should be served from the
                same URL using content negotiation. Maintenance of multiple
                formats is best achieved by generating each available format on
                the fly based on a single source of the metadata.</li>
              <li> when defining machine readable metadata, reusing existing
                standard terms and popular vocabularies are strongly
                recommended. For example, Dublin Core Metadata (DCMI) terms
                [[DC-TERMS]] and Data Catalog Vocabulary [[VOCAB-DCAT]] should
                be used to provide descriptive metadata (see Section ).</li>
            </ul>
            <!--             <label class="expand" for="example-ProvideMetadata"></label> <input
              id="example-ProvideMetadata" type="checkbox"> -->
            <aside class="example">
              <p>to be done </p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>For <em>human readable metadata</em>, check that a human user
              can understand the metadata associated with a dataset. </p>
            <p>For <em>machine readable metadata</em>, access the same URL
              either with a user agent that accepts a more data oriented format
              or a tool that extracts the data from an HTML page.<a href="http://rdf-translator.appspot.com/"><br>
              </a></p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataAvailable">R-MetadataAvailable,
                </a><a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataDocum">R-MetadataDocum,
                </a><a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataMachineRead">R-MetadataMachineRead</a></p>
          </section>
          <section class="benefits">
            <p class="subhead">Benefits</p>
            <!-- Comprehension -->
            <svg xmlns="http://www.w3.org/2000/svg" class="stamp">
              <g>
                <circle cx="26" cy="26" r="25" fill="#404040" stroke-width="2" stroke="#404040"

                  style="fill-opacity:0.4;"></circle>
                <text x="13" y="41" font-family="Arial" font-size="40" fill="#404040">C</text>
              </g> </svg> </section>
        </div>
        <!-- end of BP -->
        <!-- begin Discovery Metadata BP -->
        <div class="practice">
          <p><span id="DescriptiveMetadata" class="practicelab">Provide
              descriptive metadata</span></p>
          <p class="practicedesc">The overall features of a dataset must be
            described by metadata</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Explicitly providing dataset descriptive information allows user
              agents to automatically discover datasets available on the Web and
              it allows humans to understand the nature of the dataset. </p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>It should be possible for humans to understand the nature of the
              dataset.</p>
            <p>It should be possible for user agents be able to automatically
              discover the dataset.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p> Discovery metadata should include the following overall features
              of a dataset: </p>
            <ul>
              <li>The <strong>title</strong> and a <strong>description</strong>
                of the dataset.</li>
              <li>The <strong>keywords</strong> describing the dataset. </li>
              <li>The <strong>date of publication</strong> of the dataset. </li>
              <li> The <strong>entity responsible (publisher)</strong> for
                making the dataset available.</li>
              <li> The <strong> contact point </strong> of the dataset.</li>
              <li> The <strong>spatial coverage </strong> of the dataset.</li>
              <li> The <strong> temporal period </strong> that the dataset
                covers.</li>
              <li> The <strong> themes/categories </strong> covered by a
                dataset. </li>
              <!--<li>Any <strong>variants</strong> (e.g. different
                  human-language translations) of data.</li>                <li><strong>Access mechanisms</strong> through which the data be                  accessed (see <a href="#access">Data Access</a>). </li> -->
            </ul>
            <!-- <p> The information above should be included both in the human
                understandable and the machine understandable versions of the                metadata. </p> -->
            <p> The machine readable version of the discovery metadata may be
              provided according to the vocabulary recommended by W3C to
              describe datasets, i.e. the Data Catalog Vocabulary
              [[VOCAB-DCAT]]. This provides a framework in which datasets can be
              described as abstract entities. </p>
            <!--<p>See also <a href="#AdministrativeMetadata">Provide
                Administrative Metadata</a></p> -->
            <aside class="example">
              <h5>Machine-readable</h5>
              <p>The example below shows how to use [[VOCAB-DCAT]] to provide
                the machine readable <strong> discovery </strong> metadata for
                the timetable dataset (dataset-001). </p>
              <pre class="highlight">  :dataset-001
       a dcat:Dataset;
       dct:title "Bus timetable of MyCity";
       dcat:keyword "transport","mobility" ,"bus";
       dct:issued "2015-05-05"^^xsd:date';
       dct:modified "2015-05-05"^^xsd:date';
       dcat:contactPoint &lt;http://example.org/transport-agency/contact&gt;;
       dct:temporal &lt;http://reference.data.gov.uk/id/year/2014&gt;;
       dct:spatial &lt;http://www.geonames.org/3399415&gt;;
       dct:publisher:transport-agency-mycity ;
       dct:accrualPeriodicity &lt;http://purl.org/linked-data/sdmx/2009/code#freq-A&gt; ;       
       .
</pre>
              <p> The following paragraph was extracted from the DCAT
                specification (to review): In order to express frequency of
                update in the example above, we chose to use an instance from
                the <a href="http://www.w3.org/TR/vocab-data-cube/#dsd-cog">Content-Oriented
                  Guidelines</a> developed as part of the <abbr title="World Wide Web Consortium">W3C</abbr>
                Data Cube Vocabulary efforts. Additionally, we chose to describe
                the spatial and temporal coverage of the example dataset using
                URIs from <a href="http://www.geonames.org/">Geonames</a> and <a

                  href="http://reference.data.gov.uk/id/interval">the Interval
                  dataset</a> from data.gov.uk, respectively. A contact point is
                also provided where comments and feedback about the dataset can
                be sent. Further details about the contact point, such as email
                address or telephone number, can be provided using VCard [<cite><a

                    href="#bib-vcard-rdf" class="bibref">vcard-rdf</a></cite>].
              </p>
              <h5>Human-readable</h5>
              <p><a href="dwbp-example.html">Example page</a> with
                human-readable description of dataset is availabe.</p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <!-- <p>A manual test might simply be to use an appropriate search tool
                and check that the dataset is discoverable as expected since                that is the intended outcome. However, a more structured test                would be to ensure that the basic metadata fields listed above                are filled. </p> -->
            <p> Check that the metadata for the dataset itself includes the
              overall features of the dataset.</p>
            <p> Check if a user agent can automatically discover the dataset. </p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataAvailable">R-MetadataAvailable</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataMachineRead">R-MetadataMachineRead</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataStandardized">R-MetadataStandardized</a></p>
          </section>
          <section class="benefits">
            <p class="subhead">Benefits</p>
            <!-- Comprehension -->
            <svg xmlns="http://www.w3.org/2000/svg" class="stamp">
              <g>
                <circle cx="26" cy="26" r="25" fill="#404040" stroke-width="2" stroke="#404040"

                  style="fill-opacity:0.4;"></circle>
                <text x="13" y="41" font-family="Arial" font-size="40" fill="#404040">C</text>
              </g> </svg>
            <!-- Discoverability -->
            <svg xmlns="http://www.w3.org/2000/svg" class="stamp">
              <g>
                <circle cx="26" cy="26" r="25" fill="#7F5C46" stroke-width="2" stroke="#7F5C46"

                  style="fill-opacity:0.4;"></circle>
                <text x="13" y="41" font-family="Arial" font-size="40" fill="#7F5C46">D</text>
              </g> </svg> </section>
        </div>
        <!-- end of BP -->
        <!-- begin Locale Parameters BP -->
        <div class="practice">
          <p><span id="LocaleParametersMetadata" class="practicelab">Provide
              locale parameters metadata </span></p>
          <p class="practicedesc">Information about locale parameters (date,
            time, and number formats, language) should be described by metadata.
          </p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p> Providing locale parameters metadata helps data consumers, i.e.,
              human users and computer applications, to understand and to
              manipulate the data, improving the re-use of the data. A locale is
              a set of parameters that defines specific data aspects, such as
              language and formatting used for numeric values and dates.
              Providing information about the locality for which the data is
              currently published aids data users in interpreting its meaning.
              Date, time, and number formats can have very different meanings,
              despite similar appearances. Making the language explicit allows
              users to determine how readily they can work with the data and may
              enable automated translation services.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>It should be possible for data consumers to interpret the meaning
              of dates, times and numbers accurately by referring to locale
              information.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p> Locale parameters metadata should include the following
              information: </p>
            <ul>
              <li>The language(s) of the dataset.</li>
              <li>The formats used for numeric values, dates and time. </li>
            </ul>
            <p> The machine readable version of the discovery metadata may be
              provided according to the vocabulary recommended by W3C to
              describe datasets, i.e. the Data Catalog Vocabulary
              [[VOCAB-DCAT]].</p>
            <aside class="example">
              <p> The example below shows the machine readable metadata for
                dataset-001 with the inclusion of the <strong> locale
                  parameters </strong> metadata. </p>
              <pre class="highlight">  :dataset-001
       a dcat:Dataset;
       dct:title "Bus timetable of MyCity";
       dcat:keyword "transport","mobility" ,"bus";
       dct:issued "2015-05-05";
       dct:modified "2015-05-05";
       dcat:contactPoint &lt;http://example.org/transport-agency/contact&gt;;
       dct:temporal &lt;http://reference.data.gov.uk/id/year/2014&gt;;
       dct:spatial &lt;http://www.geonames.org/3399415&gt;;
       dct:publisher:transport-agency-mycity ;
       dct:accrualPeriodicity &lt;http://purl.org/linked-data/sdmx/2009/code#freq-A&gt;; 
       <strong>dct:language &lt;http://lexvo.org/id/iso639-3/eng&gt;;</strong>
       <strong>dct:language &lt;http://lexvo.org/id/iso639-3/por&gt;;</strong>
       .
</pre>
              <p> To declare the languages the dataset is published in use
                dct:language. If the dataset is available in mutiple languages,
                use multiple values for this property [[VOCAB-DCAT]. As proposed
                in Dataset Descriptions from HCLS Community [[HCLS]] values were
                taken from the Lexvo.org Ontology [[Lexvo]]. </p>
              <div class="note"> to include date and numbers formats! </div>
              <div class="issue"> DCAT has a property to describe language, but
                there are no properties to describe date, time and numeric
                formats. Which vocabulary should be used to provide this type of
                metadata? This is <a href="https://www.w3.org/2013/dwbp/track/issues/167">Issue-167</a>.</div>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p> Check that the metadata for the dataset itself includes the
              language in which it is published and that all numeric, date, and
              time fields have locale metadata provided either with each field
              or as a general rule. </p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-FormatLocalize">R-FormatLocalize</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataAvailable">R-MetadataAvailable</a></p>
          </section>
          <section class="benefits">
            <p class="subhead">Benefits</p>
            <!-- Comprehension -->
            <svg xmlns="http://www.w3.org/2000/svg" class="stamp">
              <g>
                <circle cx="26" cy="26" r="25" fill="#404040" stroke-width="2" stroke="#404040"

                  style="fill-opacity:0.4;"></circle>
                <text x="13" y="41" font-family="Arial" font-size="40" fill="#404040">C</text>
              </g> </svg> </section>
        </div>
        <!-- end of Locale Parameters BP -->
        <!-- begin of Provide Structural Metadata -->
        <div class="practice">
          <p><span id="StructuralMetadata" class="practicelab">Provide
              structural metadata </span> </p>
          <p class="practicedesc"> Information about the schema and internal
            structure of a distribution must be described by metadata</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p> Providing information about the internal structure of a
              distribution can be helpful when exploring or querying the
              dataset. Besides, structural metadata provides information that
              helps to understand the meaning of the data. </p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>It should be possible for humans to understand the internal
              structure or schema of a distribution. </p>
            <p> It should be possible for user agents be able to automatically
              process the structural metadata about a distribution.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p> Structural metadata is available according to the format of a
              specific distribution and it may be provided within separate
              documents or embedded into the document. For more details see the
              links below. </p>
            <ul>
              <li> Tabular data: see <a href="http://www.w3.org/TR/2015/WD-tabular-data-model-20150416/#locating-metadata">
                  Model for Tabular Data and Metadata on the Web </a> </li>
              <li> JSON-LD: see <a href="http://www.w3.org/TR/json-ld/">
                  JSON-LD 1.0 </a> </li>
              <li> XML: see <a href="http://www.w3.org/XML/Schema"> XML Schema
                </a> </li>
            </ul>
            <aside class="example">
              <p>to be done </p>
            </aside>
          </section>
          <section class="test"><span style="font-weight: bold;">How to Test </span>
            <p> Check that the distribution itself includes structural
              information about the data organization. </p>
            <p> Check if a user agent can automatically process the structural
              information about the distribution.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataAvailable">R-MetadataAvailable</a></p>
          </section>
        </div>
        <!-- end of BP --> </section>
      <!-- begin of Data Licenses -->
      <section id="licenses">
        <h2>Data Licenses</h2>
        <p>A license is a very useful piece of information to be attached to
          data on the Web. As defined by the Dublin Core Metadata Initiative
          [[DC-TERMS]], a license is a legal document giving official permission
          to do something with the data with which it is associated. According
          to the type of license adopted by the publisher, there might be more
          or fewer restrictions on sharing and re-using data. In the context of
          data on the Web, the license of a dataset can be specified within the
          data, or outside of it, in a separate document to which it is linked.
          <!--However, in line with the Linked
          Data principles, licenses for such datasets should          be specified in RDF, for instance through the Dublin Core vocabulary          [[DC-TERMS]].</p> -->
          <!-- begin of Provide Data License BP --> </p>
        <!-- begin of machine detectable license BP -->
        <div class="practice">
          <p><span id="DataLicense" class="practicelab">Provide data license
              information</span></p>
          <p class="practicedesc"> Data license information should be available
          </p>
          <div class="axioms">
            <p class="subhead">Why</p>
            <p>The presence of license information is essential for data
              consumers to assess the usability of data. User agents, for
              example, may use the presence/absence of license information as a
              trigger for inclusion or exclusion of data presented to a
              potential consumer. </p>
            <!--Even though the license may be presented in natural
                language, where data links to the URL of a well known license,                the user agent may be able to present the well known features to                the potential consumer.-->
          </div>
          <div class="description">
            <p class="subhead">Intended outcome</p>
            <p>It should be possible for humans to understand possible
              restrictions placed on the use of a distribution.</p>
            <p>It should be possible for machines to automatically detect the
              data license of a distribution.</p>
          </div>
          <div class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>The machine readable version of the data license metadata may be
              provided using one of the following vocabularies that include
              properties for linking to a license: </p>
            <ul>
              <li> Dublin Core [[DC-TERMS]]</li>
              <li> Creative Commons [[CC-VOCAB]] </li>
              <li> schema.org [[SCHEMA-ORG]]</li>
              <li> XHTML [[XHTML-VOCAB]]</li>
            </ul>
            There are also a number of machine readable rights languages,
            including:
            <ul>
              <li> The Creative Commons Rights Expression Language [[ccREL]]</li>
              <li> The Open Data Rights Language [[ODRL]]</li>
              <li> The Open Data Rights Statement Vocabulary [[ODRS]].</li>
            </ul>
            <!-- <p>Links to the license can be provided from the data itself, from
                an HTML page that describes the data (via a Link element), or                via an HTTP Link Header, the latter two with a <code>@rel</code>                value of <code>license</code>. </p>                Further information about open data licensing can be found in                the Publisher's Guide to Open Data Licensing, published by the                Open Data Institute [[ODI-LICENSING]]. -->
            <aside class="example">
              <p> </p>
              <p> The example below shows the machine readable metadata for
                :dataset-001-csv with the inclusion of the <strong> data
                  license </strong> information. </p>
              <pre class="highlight">  :dataset-001-csv
       a dcat:Distribution;
       dcat:mediaType "text/csv";
       dct:license &lt;http://creativecommons.org/licenses/by-sa/3.0/&gt;;
       .
</pre> </aside>
          </div>
          <div class="test">
            <p class="subhead">How to Test</p>
            <p> Check that the metadata for the dataset itself includes the data
              license information. </p>
            <p> Check if a user agent can automatically detect the data license
              of the dataset.</p>
            <!-- <p>Check for the presence of one or more of:</p>
              <ol>                <li>the presence of an RDF predicate;</li>                <li>an HTML Link element;</li>                <li>an HTTP Link header;</li>              </ol>              <p>that links the dataset to a license and/or rights information.</p>              <div class="issue"> I think we should have a standard way to test                the presence of machine readable metadata. This test is very                different from the one proposed for the discovery metadata. </div>            </div> -->
            <div class="ucr">
              <p class="subhead">Evidence</p>
              <p><span>Relevant use cases</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-LicenseAvailable">R-LicenseAvailable</a>
                and <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataMachineRead">R-MetadataMachineRead</a>
              </p>
            </div>
          </div>
          <!-- end of machine detectable license BP --> </div>
      </section>
      <!--      <div class="issue">
        <p> I am not sure of how to define data licence metadata. In DCAT,          licence is defined as a property of distribution. However, in our BP          we just mention datasets. Should we also consider distributions? </p>      </div>-->
      <!--      <div class="example">
          <p> </p>          <p> The example below shows the machine readable metadata for            dataset-001 with the inclusion of the <strong> data licence </strong>            metadata. </p>          <pre>  :dataset-001       a dcat:Dataset;       dct:title "Bus timetable of MyCity";       dcat:keyword "transport","mobility" ,"bus";       dct:issued "2015-05-05";       dct:modified "2015-05-05";       dcat:contactPoint &lt;http://example.org/transport-agency/contact&gt;;       dct:temporal &lt;http://reference.data.gov.uk/id/year/2014&gt;;       dct:spatial &lt;http://www.geonames.org/3399415&gt;;       dct:publisher:transport-agency-mycity ;       dct:accrualPeriodicity &lt;http://purl.org/linked-data/sdmx/2009/code#freq-A&gt; ;        dct:language &lt;http://id.loc.gov/vocabulary/iso639-1/en&gt; ;       <strong>dct:licence &lt;http://creativecommons.org/licenses/by-sa/3.0/&gt; ;</strong>&lt;http://creativecommons.org/licenses/by-sa/3.0/&gt;       .</pre>                 </div> -->
      <!-- end of Data Licenses -->
      <!-- begin of Data Provenance -->
      <section id="provenance">
        <h3>Data Provenance</h3>
        <p>Provenance originates from the French term "provenir" (to come from),
          which is used to describe the curation process of artwork as art is
          passed from owner to owner. Data provenance, in a similar way, is
          metadata that allows data providers to pass details about the data
          history to data users. Provenance becomes particularly important when
          data is shared between collaborators who might not have direct contact
          with one another either due to proximity or because the published data
          outlives the lifespan of the data provider projects or organizations.</p>
        <p>The Web brings together business, engineering, and scientific
          communities creating collaborative opportunities that were previously
          unimaginable. The challenge in publishing data on the Web is providing
          an appropriate level of detail about its origin. The data publishers
          may not necessarily be the data provider and so collecting and
          conveying this corresponding metadata is particularly important.
          Without provenance, consumers have no inherent way to trust the
          integrity and credibility of the data being shared. Data publishers in
          turn need to be aware of the needs of prospective consumer communities
          to know how much provenance detail is appropriate. </p>
        <!-- <p>To meet this need, the W3C community offers the Provenance Ontology
            [[PROV-O]] so that data publishers can formally describe data            provenance and methodologies so that consumers can query and use            provenance information. </p> -->
        <!-- begin of Provide Data Provenance BP -->
        <div class="practice">
          <p><span id="DataProvenance" class="practicelab">Provide data
              provenance information</span></p>
          <p class="practicedesc">Data provenance information should should be
            available. </p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Without accessible data provenance, data consumers will not know
              the origin or history of the published data.</p>
            <!-- <p>Data provenance is metadata that corresponds to data. Data
                provenance relies upon existing vocabularies that make                provenance easily identifiable such as the Provenance Ontology                [[PROV-O]].</p> -->
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <!--<p>Data published on the Web should include, or link to,
                provenance information.</p> -->
            <p>It should be possible for humans to know the origin or history of
              the dataset.</p>
            <p>It should be possible for machines to automatically process the
              provenance information about the dataset.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>The machine readable version of the data provenance may be
              provided according to the ontology recommended by W3C to describe
              provenance information, i.e., the Provenance Ontology [[PROV-O]].
            </p>
            <!--
              <ol>                <li>Use an appropriate level of detail that will be meaningful                  to the intended audience. </li>                <li>Write the data provenance in either a machine readable form                  such as Turtle or RDF/XML or embed provenance in an HTML page                  using [[JSON-LD]], or [[HTML-RDFA]] </li>                <li>Verify that the data provenance references published data</li>              </ol> -->
            <aside class="example">
              <p> The example below shows the machine readable metadata for
                dataset-001 with the inclusion of the <strong>provenance</strong>
                metadata. </p>
              <pre class="highlight">:dataset-001
  a dcat:Dataset, prov:Entity;
  dct:title "Bus timetable of MyCity";
  dcat:keyword "transport","mobility" ,"bus";
  dct:issued "2015-05-05";
  dct:modified "2015-05-05";
  dcat:contactPoint &lt;http://example.org/transport-agency/contact&gt;;
  dct:temporal &lt;http://reference.data.gov.uk/id/year/2014&gt;;
  dct:spatial &lt;http://www.geonames.org/3399415&gt;;
  dct:publisher:transport-agency-mycity ;
  dct:accrualPeriodicity &lt;http://purl.org/linked-data/sdmx/2009/code#freq-A&gt; ; 
  dct:language &lt;http://id.loc.gov/vocabulary/iso639-1/en&gt; ;
  <strong>prov:wasAttributedTo :john; </strong> 
  .
       
:john
  a foaf:Person, prov:Agent;
  foaf:givenName "John";
  foaf:mbox &lt;mailto:john@mycitytransport.org&gt;;
  prov:actedOnBehalfOf :transport-agency-mycity;
 .
:transport-agency-mycity
  a foaf:Organization, prov:Agent;
  foaf:name "Transport Agency of Mycity";
  .
</pre> </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p> Check that the metadata for the dataset itself includes the
              provenance information about the dataset.</p>
            <p> Check if a computer application can automatically process the
              provenance information about the dataset. </p>
            <!--<p>The PROV Implementation Report [[PROV-IMP]] lists a number of
                validator tools that can be used to test for the presence of                provenance information provided using [[PROV-O]].</p> -->
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-ProvAvailable">R-ProvAvailable</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataAvailable">R-MetadataAvailable</a>
            </p>
          </section>
        </div>
        <!-- end of Provide Data Provenance BP --> </section>
      <!-- end of Data Provenance -->
      <!-- begin of Data Quality -->
      <section id="quality">
        <h3>Data Quality</h3>
        <p>Data quality is commonly defined as ‚Äúfitness for use‚Äù for a specific
          application or use case. It can affect the potentiality of the
          application that use data, as a consequence, its inclusion in the data
          publishing and consumption pipelines is of primary importance.</p>
        <p>Usually, the assessment of quality involves different kinds of
          quality dimensions, each representing groups of characteristics that
          are relevant to publishers and consumers. Measures and metrics are
          defined to assess the quality for each dimension. There are heuristics
          designed to fit specific assessment situations that rely on quality
          indicators, namely, pieces of data content, pieces of data
          meta-information, and human ratings that give indications about the
          suitability of data for some intended use.</p>
        <!--<p>Dimensions and metrics to adopt might largely depend on the
            specific application scenario, or even on the data domain. A            systematic review of dimensions, metrics adopted in the context of            Linked Open Data can be found in the recent literature (e.g., see            [[ZAVERI]]). </p> -->
        <!-- begin of Provide Data Quality BP -->
        <div class="practice">
          <p><span id="DataQuality" class="practicelab">Provide data quality
              information</span></p>
          <p class="practicedesc">Data Quality information should be available.</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Data quality might seriously affect the suitability of data for
              specific applications, including applications very different from
              the purpose for which it was originally generated. Documenting
              data quality significantly eases the process of datasets
              selection, increasing the chances of re-use. Independently from
              domain-specific peculiarities, the quality of data should be
              documented and known quality issues should be explicitly stated in
              metadata.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>It should be possible for humans to have access to information
              that describes the quality of the dataset.</p>
            <p>It should be possible for machines to automatically process the
              quality information about the dataset.</p>
            <!--<p> Information about the quality of the data should be provided
                for humans. Ideally it is also made available in machine                readable manner for processing by applications.</p> -->
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <!-- <p>Depending on the application domain, information pertaining to
                the quality may rely on specific quality metrics or                feedback-opinion. Specific quality metadata fields may or may                not be explicitly included in the metadata vocabularies adopted                by catalogs. Independently from domain-specific peculiarities,                the quality of data should be documented and known quality                issues should be explicitly stated in metadata. </p> -->
            <!--            <p>The definition of a Quality Vocabulary is included in the
              activity of the DWBP group in order to support in the              implementation of this best practice. The Quality Vocabulary is              foreseen as an extension to DCAT to cover the quality of the data,              how frequently is it updated, whether it accepts user corrections,              persistence commitments etc. When used by publishers, this              vocabulary will foster trust in the data amongst developers. </p> -->
            <p>The machine readable version of the dataset quality metadata may
              be provided according to the vocabulary that is being developed by
              the <abbr title="Data on the Web Best Practices">DWBP</abbr>
              working group , i.e., the Data Quality Vocabulary [[DQV]]. </p>
            <aside class="example">
              <p>to be done </p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p> Check that the metadata for the dataset itself includes quality
              information about the dataset.</p>
            <p> Check if a computer application can automatically process the
              quality information about the dataset. </p>
            <!--<p>Check whether the metadata explicitly includes a description of
                the quality of the data.</p> --> </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p>Information about the relevance of the BP is described by
              requirements documented in <a href="http://www.w3.org/TR/dwbp-ucr/">the
                Data on the Web Best Practices Use Cases &amp; Requirements
                document</a>: <a href="http://www.w3.org/TR/dwbp-ucr/#requirements-for-quality-and-granularity-description-vocabulary">Requirements
                for Data Quality</a></p>
          </section>
          <!-- end of Provide Data Quality BP -->
          <!--<div class="issue"> Should we provide more specific/ detailed
            strategies on how to attach quality info in metadata apart from the            use of the data quality vocabulary the group is working on? <a href="https://www.w3.org/2013/dwbp/track/issues/116">Issue-116</a>-->
        </div>
      </section>
      <!-- end of Data Quality -->
      <!-- begin of Data Versioning -->
      <section id="dataVersioning">
        <h3>Data Versioning</h3>
        <!--div class="issue"> This section provides an initial idea of how we are
          planning to deal with versioning. We're keen to hear comments about          our proposal to represent the relationship between a dataset and its          different versions  </div>        <!-- <div class="issue"> To discuss if the items in yellow each represent a          different dataset (they report different data points) or a different          version, if released independently. This is <a href="https://www.w3.org/2013/dwbp/track/issues/193">Issue-193</a>.</div> -->
        <!--<div class="issue"> To debate if versions attempt to report the same
          data. This is <a href="https://www.w3.org/2013/dwbp/track/issues/193">Issue-193</a>.</div> -->
        <p> Datasets published on the Web may change over time. Some datasets
          are updated on a schedule basis and other datasets are changed as
          improvements in collecting the data make updates worthwhile. In order
          to deal with these changes, new versions of a dataset may be created.
          Dataset versioning has been the subject of numerous discussions,
          however there is no consensus about when creating a new version of a
          datset. In the following we provide some guidance about when to create
          a new dataset version, but the publisher should feel free to decide
          when to create a new dataset or a new version of an existing dataset.
        </p>
        <p> In general, a dataset may change with respect its data as well as
          the metadata that describes it. Considering data updates, we may say
          that a dataset B is considered a version of an existing dataset A,
          when A and B store values for the same observation about the world and
          dataset B gives new values to such observation. These new values may
          result, for example, from the updating of existing data or the
          inclusion of new data. To illustrate this definition, consider the
          following scenarios related to a dataset that collects data about bus
          timetables:</p>
        <ul>
          <li> Scenario 1: a new bus stop is created and its timetable doesn‚Äôt
            exist on the dataset;</li>
          <li> Scenario 2: an existing bus stop is removed and its timetable
            should be deleted from the dataset;</li>
          <li> Scneario 3: an error was identified in one of the existing
            timetables stored in the dataset and this error must be corrected;</li>
        </ul>
        <p> In all scenarios, a new dataset, i.e. a new version of the existing
          dataset, should be created to reflect the corresponding update.</p>
        <p> Considering metadata updates, we should evaluate the impact of the
          update on the data in order to choose between the creation of a new
          dataset or just the updating of the dataset description. For example,
          an update on the structural metadata of a dataset may result in the
          creation of a new dataset while the updating of descriptive metadata
          requires just the modification of the current dataset definition. In
          the first case, structural changes will have an impact on how the data
          is being described and then a new dataset should be created.</p>
        <p> It is important to note that the creation of multiple datasets to
          represent time series as well as spatial series, e.g. the same kind of
          data for different regions, are not considered as multiple versions
          for the same dataset. In this case, each dataset covers a different
          observation about the world and should be treated as a new dataset
          instead of a new version of an existing dataset. This is the case of a
          dataset that collects data about weakly weather forecast of a given
          city, where every week a new dataset should be created to store data
          about that specific week. </p>
        <p> Even for small changes it is important to keep track of the
          different dataset versions to make the dataset trustworthy. Publishers
          should remember that a given dataset may be in use for one or more
          data consumers and they should be notified about the creation of new
          versions or it should be possible to automatically identify different
          versions of the same dataset. All these types of dataset updates need
          a consistent, informative approach to versioning, so data consumers
          can understand and work with the changing data. </p>
        <!--
        <p>Data on the Web often changes over time. Many datasets are updated on          a scheduled basis, such as census data or funding data that changes          every fiscal year. Other datasets are changed as improvements in          collecting the data make updates worthwhile. Still other data changes          in real time or near real time. All these types of data need a          consistent, informative approach to versioning, so data consumers can          understand and work with the changing data. </p>        <p>In order to deal with changes over time, multiple versions may be          created for a single dataset. To illustrate this let us consider a          simple example: a dataset that collects data about weekly weather          forecast for MyCity. The following figure shows the relation between          the dataset (Dataset001) and its different versions (Dataset001_W1,          Dataset001_W2, Dataset001_W3 and Dataset001_W4), where each version          corresponds to the weather forecast of a week of May 2015. Each          dataset version has two different distributions: one in CSV and one in          JSON. For example, the version Dataset001_W1 has two distributions:          Dataset001_W1_csv and Dataset001_W1_json. </p>        <figure id="contextDiagram"> <img alt="A single node on the left hand side is the Dataset, lines from here split to link to 4 versions of the dataset, each of which is linked to two distributions of each one, making a total of 8 files for this one dataset."            src="images/DWBP_Context.svg"> <figcaption>Diagram showing the            relationships between Dataset, Versions and Distributions</figcaption>        </figure> -->
        <p>The following best practices address issues that arise in tracking
          and managing different versions of datasets.</p>
        <p><br>
        </p>
        <!-- begin of provide Versioning Info BP -->
        <div class="practice">
          <p><span id="VersioningInfo" class="practicelab">Provide versioning
              information</span></p>
          <p class="practicedesc">Information about dataset versioning should be
            available.</p>
          <!-- <p class="practicedesc">Data that will be updated over time should
              be assigned a unique version number or, at a minimum, a unique              version date, and that unique identifier should              be distributed with the data in the form of metadata. </p> -->
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Version information makes a dataset uniquely identifiable.
              Uniqueness can be used by data consumers to determine how data has
              changed over time and to determine specifically which version of a
              dataset they are working with. Good data versioning enables
              consumers to understand if a newer version of a dataset is
              available. Explicit versioning allows for repeatability in
              research, enables comparisons, and prevents confusion. Using
              unique version numbers that follow a standardized approach can
              also set consumer expectations about how the versions differ.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>It should be possible for data consumers to easily determine
              which version of the dataset they are working with.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <!-- <p>Providing version information is a topic of <a href="https://lists.w3.org/Archives/Public/public-vocabs/2015Jan/0120.html"
                  title="A versioning model for schema.org">much debate</a>. -->
            <p> The precise method adopted for providing versioning information
              may vary according to the context, however there are some basic
              guidelines that can be followed, for example: </p>
            <ul>
              <li>Include a unique version number as part of the metadata for
                the dataset. </li>
              <li>Use a consistent numbering scheme with a meaningful approach
                to incrementing digits, such as [[SchemaVer]]. </li>
              <!-- <li>Provide a description of what has changed since the previous
                version. </li> -->
              <li>If the data is made available through an API, the URI used to
                request the latest version of the data should not change as the
                versions change, but it should be possible to request a specific
                version through the API.
                <!--See <a href="#VersioningVocabularies">Vocabulary
                  versioning</a> for more on assigning stable URIs for the 'latest version' and for each snapshot.</p> -->
              </li>
              <li> Use the [[Memento protocol]], or components thereof, to
                express temporal versioning of a dataset and to access the
                version that was operational at a given datetime. The Memento
                protocol aligns closely with the approach for assigning URIs to
                versions described in the following and used for W3C
                specifications.</li>
            </ul>
            <p>The Web Ontology Language [[OWL2-QUICK-REFERENCE]] and the
              Provenance, authoring and versioning Ontology [[PAV]] provides a
              number of annotation properties for version information.</p>
            <aside class="example">
              <p> The example below shows the machine readable metadata for
                dataset-001 with the inclusion of the <strong>versioning</strong>
                metadata. </p>
              <pre class="highlight">:dataset-001
  a dcat:Dataset, prov:Entity, version:VersionedThing;
  dct:title "Bus timetable of MyCity";
  dcat:keyword "transport","mobility" ,"bus";
  dct:issued "2015-05-05";
  dct:modified "2015-05-05";
  dcat:contactPoint &lt;http://example.org/transport-agency/contact&gt;;
  dct:temporal &lt;http://reference.data.gov.uk/id/year/2014&gt;;
  dct:spatial &lt;http://www.geonames.org/3399415&gt;;
  dct:publisher:transport-agency-mycity ;
  dct:accrualPeriodicity &lt;http://purl.org/linked-data/sdmx/2009/code#freq-A&gt; ; 
  dct:language &lt;http://id.loc.gov/vocabulary/iso639-1/en&gt; 
  prov:wasAttributedTo :john;
  <strong>owl:versionInfo "1.0"; </strong> </pre>
              <p><em>Using Memento</em></p>
              Assume:
              <ul>
                <li> http://example.org/dataset is the ‚Äúgeneric URI‚Äù at which
                  the current version of a dataset is always available </li>
                <li> http://example.org/dataset-002 is the versioned URI for the
                  current dataset </li>
                <li> http://example.org/dataset-001 is the versioned URI of the
                  prior version of the dataset </li>
                <li> http://example.org/dataset-000 is the versioned URI of the
                  first version of the dataset </li>
              </ul>
              <p> In the Memento protocol, the versioned URIs provide HTTP
                response header information to express their version datetime
                and their relation to the generic URI: </p>
              <pre class="highlight">curl -I http://example.org/dataset-001 

HTTP/1.1 200 OK
Memento-Datetime: Sun, 05 April 2015 00:00:00 GMT
Link: &lt;http://example.org/dataset&gt;; rel=‚Äúoriginal‚Äù </pre>
              <p> The versioned URIs can provide a link to a TimeGate, which
                supports datetime negotiation as a means to determine which
                version of a dataset was operational at a given datetime: </p>
              <pre class="highlight"> 
curl -I http://example.org/dataset-001

HTTP/1.1 200 OK
Memento-Datetime: Sun, 05 April 2015 00:00:00 GMT
Link:&lt;http://example.org/dataset&gt;; rel=‚Äúoriginal‚Äù,
&lt;http://example.org/timegate/dataset&gt;; rel=‚Äútimegate‚Äù </pre>
              <p> The generic URI can also provide a link to a TimeGate: </p>
              <pre class="highlight">curl -i -H http://example.org/dataset

HTTP/1.1 200 OK
Link: &lt;http://example.org/timegate/dataset&gt;; rel=‚Äútimegate‚Äù </pre>
              <p>This is how a client determines which dataset version was
                operational on March 20 2015: </p>
              <pre class="highlight"> 
curl -I -H "Accept-Datetime: Fri, 20 Mar 2015  12:00:00 GMT" http://example.org/timegate/dataset

HTTP/1.1 302 Found
Vary: accept-datetime
Location: http://example.org/dataset-000
Link: &lt;http://example.org/dataset&gt; rel="original" </pre>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>Check that a unique version number or date is provided with the
              metadata describing the dataset.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-DataVersion">R-DataVersion</a></p>
          </section>
        </div>
        <!-- end of provide Versioning Info BP -->
        <!-- begin of provide version history BP -->
        <div class="practice">
          <p><span id="VersionHistory" class="practicelab">Provide version
              history</span></p>
          <p class="practicedesc">A version history about the dataset should be
            available.</p>
          <!-- <p class="practicedesc">A version history should
              be available for versioned data.</p> -->
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>In creating applications that use data, it can be helpful to
              understand the variability of that data over time. Interpreting
              the data is also enhanced by an understanding of its dynamics.
              Determining how the various versions of a dataset differ from each
              other is typically very laborious unless a summary of the
              differences is provided.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>It should be possible for data consumers to understand how the
              dataset typically changes from version to version and how any two
              specific versions differ.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>Provide a list of published versions and a description for each
              version that explains how it differs from the previous version. An
              API can expose a version history with a single dedicated URL that
              retrieves the latest version of the complete history.</p>
            <div class="issue"> Which vocabulary should be used to describe the
              versioning history? This is <a href="https://www.w3.org/2013/dwbp/track/issues/168">Issue-168</a></div>
            <aside class="example">
              <p> Suppose that a new bus stop was created and its timetable
                doesn't exist on the dataset-001. To maintain the dataset-001 up
                to date a new dataset (dataset-002) was created. dataset-002
                includes all the data from dataset-001 plus the data about the
                new bus stop. The machine readable metadata of dataset-002 is
                shown below. </p>
              <pre class="highlight">:dataset-002
       a dcat:Dataset, version:Version;
       dct:title "Bus timetable of MyCity";
       dcat:keyword "transport","mobility" ,"bus";
       dct:issued "2015-05-06"^^xsd:date';
       dct:modified "2015-05-06"^^xsd:date';
       dcat:contactPoint &lt;http://example.org/transport-agency/contact&gt;;
       dct:temporal &lt;http://reference.data.gov.uk/id/year/2014&gt;;
       dct:spatial &lt;http://www.geonames.org/3399415&gt;;
       dct:publisher:transport-agency-mycity ;
       dct:accrualPeriodicity &lt;http://purl.org/linked-data/sdmx/2009/code#freq-A&gt; ; 
       dct:language &lt;http://id.loc.gov/vocabulary/iso639-1/en&gt;
       ...
       <strong>dct:isVersionOf:dataset-001;
       pav:previousVersion:dataset-001;
       rdfs:comment "A new timetable was included in the dataset-001 to reflect the creation of a new bus stop."
       owl:versionInfo "1.1". </strong>
</pre>
              <p> Versioning metadata for dataset-001 after the creation of
                dataset-002, which is a new version of dataset-001. </p>
              <pre class="highlight">:dataset-001
       a dcat:Dataset, version:VersionedThing;
       dct:title "Bus timetable of MyCity";
       <!-- dcat:keyword "transport","mobility" ,"bus";
       dct:issued "2015-05-05"^^xsd:date';
       dct:modified "2015-05-05"^^xsd:date';
       dcat:contactPoint &lt;http://example.org/transport-agency/contact&gt;;
       dct:temporal &lt;http://reference.data.gov.uk/id/year/2014&gt;;
       dct:spatial &lt;http://www.geonames.org/3399415&gt;;
       dct:publisher:transport-agency-mycity ;
       dct:accrualPeriodicity &lt;http://purl.org/linked-data/sdmx/2009/code#freq-A&gt; ; 
       dct:language &lt;http://id.loc.gov/vocabulary/iso639-1/en&gt; 
       prov:wasAttributedTo:john; -->
       ...
       <strong>version:currentVersion:dataset-002;
       dct:hasVersion :dataset-002;
       owl:versionInfo "1.0". </strong> </pre>
              <p> Using Memento:</p>
              Assume:
              <ul>
                <li> <code>http://example.org/dataset</code> is the ‚Äúgeneric
                  URI‚Äù at which the current version of a dataset is always
                  available </li>
                <li> <code>http://example.org/dataset-002</code> is the
                  versioned URI for the current dataset </li>
                <li> <code>http://example.org/dataset-001</code> is the
                  versioned URI of the prior version of the dataset </li>
                <li> <code>http://example.org/dataset-000</code> is the
                  versioned URI of the first version of the dataset </li>
              </ul>
              <p> The versioned URIs, the generic URI, and the TimeGate can
                provide a link to a TimeMap that provides an overview of all
                temporal versions of the dataset: </p>
              <pre class="highlight">curl -I http://example.org/dataset-001

HTTP/1.1 200 OK
Date: Sat, 24 Oct 2015 16:23:30 GMT
Memento-Datetime: Sun, 05 April 2015 00:00:00 GMT
Link: &lt;http://example.org/dataset&gt;; rel=‚Äúoriginal‚Äù,
 &lt;http://example.org/timemap/dataset&gt;; rel=‚Äútimemap‚Äù;
type="application/link-format"

curl -I http://example.org/timemap/dataset

HTTP/1.1 200 OK
Content-Type: application/link-format

&lt;http://example.org/dataset&gt;;rel="original‚Äù,
&lt;http://example.org/timedate/dataset&gt;;rel="timegate‚Äù,
&lt;http://example.org/timedate/dataset&gt;;rel="timemap‚Äù;
type="application/link-format",
&lt;http://example.org/dataset-000&gt;; rel=‚Äúfirst memento"; datetime="Thu,05 Mar 2015 00:00:00 GMT",
&lt;http://example.org/dataset-002&gt;; rel=‚Äúmemento"; datetime=‚ÄúSun, 05 Apr2015 00:00:00 GMT"
&lt;http://example.org/dataset-002&gt;; rel=‚Äúlast memento"; datetime="Tue,05 May 2015 00:00:00 GMT"
</pre>
              <p> The versioned URI can provide information regarding relations
                with other dataset versions: </p>
              <pre class="highlight">curl -I http://example.org/dataset-001

HTTP/1.1 200 OK
Memento-Datetime: Sun, 05 April 2015 00:00:00 GMT
Link: &lt;http://example.org/dataset&gt;; rel=‚Äúoriginal‚Äù,
&lt;http://example.org/dataset-000&gt;; rel=‚Äúprev first memento";
datetime="Thu, 05 Mar 2015 00:00:00 GMT",
&lt;http://example.org/dataset-002&gt;; rel=‚Äúnext last memento";
datetime="Tue, 05 May 2015 00:00:00 GMT"
</pre> </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>Check that a list of published versions is available, and that
              each version is described.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-DataVersion">R-DataVersion</a></p>
          </section>
        </div>
        <!-- end of provide version history BP -->
        <!-- end of Metadata --> </section>
      <section id="DataIdentifiers">
        <h3>Data Identifiers</h3>
        <p>Identifiers take many forms and are used extensively in every
          information system. Data discovery, usage and citation on the Web
          depends fundamentally on the use of HTTP (or HTTPS) URIs: globally
          unique identifiers that can be looked up by dereferencing them over
          the Internet [[RFC3986]]. It is perhaps worth emphasizing some key
          points about URIs in the current context.</p>
        <ol>
          <li>URIs are 'dumb strings', that is, they carry no semantics. Their
            function is purely to identify a resource.</li>
          <li>Although the previous point is accurate, it would be perverse for
            a URI such as http://example.com/dataset.csv to return anything
            other than a CSV file. Human readability is helpful.</li>
          <li>When de-referenced (looked up), a single URI may offer the same
            resource in more than one format. http://example.com/dataset may
            offer the same data in, say, CSV, JSON and XML. The server returns
            the most appropriate format based on <a href="http://www.w3.org/Protocols/HTTP/Negotiation">
              content negotiation </a>.</li>
          <li>One URI may redirect to another.</li>
          <li>De-referencing a URI triggers a computer program to run on a
            server so that the URI acts as a call to an API. The server may
            therefore do something as simple as return a single, static file, or
            it may carry out complex processing. Precisely what processing is
            carried out, i.e. the software on the server, is completely
            independent of the URI itself.</li>
        </ol>
        <!-- begin of Data Identification BP -->
        <div class="practice">
          <p><span id="UniqueIdentifiers" class="practicelab">Use persistent
              URIs as identifiers of datasets</span></p>
          <p class="practicedesc">Datasets must be identified by a persistent
            URI. </p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Adopting a common identification system enables basic data
              identification and comparison processes by any stakeholder in a
              reliable way. They are an essential pre-condition for proper data
              management and re-use.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>Datasets or information about datasets, must be discoverable and
              citable through time, regardless of the status, availability or
              format of the data.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>To be persistent, URIs must be designed as such and backed up by
              organizational commitments. A lot has been written on this topic
              as the table below shows. </p>
            <table id="uripatternstable">
              <caption>Some sources of information related to URI persistence</caption>
              <tbody>
                <tr>
                  <th>Status</th>
                  <th>Title</th>
                  <th>Authors and Date</th>
                </tr>
                <tr>
                  <td rowspan="3">Background</td>
                  <td><a href="http://www.w3.org/Provider/Style/URI.html">Cool
                      URIs don't change</a></td>
                  <td>Tim Berners-Lee, 1998</td>
                </tr>
                <tr>
                  <td><a href="http://www.w3.org/TR/cooluris/">Cool URIs for the
                      Semantic Web</a></td>
                  <td>Leo Saurman, Richard Cyganiak, 2008</td>
                </tr>
                <tr>
                  <td><a href="http://www.w3.org/DesignIssues/LinkedData.html">Linked
                      Data</a></td>
                  <td>Tim Berners-Lee, 2009</td>
                </tr>
                <tr>
                  <td>Key Source</td>
                  <td><a href="http://www.cabinetoffice.gov.uk/sites/default/files/resources/designing-URI-sets-uk-public-sector.pdf">Designing
                      URI Sets for the UK Public Sector</a> (PDF)</td>
                  <td>UK Chief Technology Officer Council October 2009</td>
                </tr>
                <tr>
                  <td>Survey &amp; summary of techniques</td>
                  <td><a href="http://philarcher.org/diary/2013/uripersistence/">Study
                      on Persistent URIs</a></td>
                  <td>Phil Archer, Nikos Loutas, Stijn Goedertier, Saky
                    Kourtidis, 2013</td>
                </tr>
                <tr>
                  <td rowspan="4">Expansion</td>
                  <td><a href="http://www.jenitennison.com/blog/node/136">Creating
                      Linked Data</a></td>
                  <td>Jeni Tennison, 2009</td>
                </tr>
                <tr>
                  <td><a href="http://linkeddatabook.com/editions/1.0/">Linked
                      Data: Evolving the Web into a Global Data Space</a></td>
                  <td>Tom Heath &amp; Christian Bizer, 2011</td>
                </tr>
                <tr>
                  <td><a href="http://patterns.dataincubator.org/book/">Linked
                      Data Patterns</a></td>
                  <td>Leigh Dodds &amp; Ian Davis, 2012</td>
                </tr>
                <tr>
                  <td><a href="http://www.multilingualweb.eu/en/documents/dublin-workshop/dublin-program#linking">Best
                      Practices for Multilingual Linked Open Data</a></td>
                  <td>Jose Emilio Labra Gayo, 2012</td>
                </tr>
                <tr>
                  <td>Detail</td>
                  <td><a href="http://csarven.ca/statistical-linked-dataspaces">Statistical
                      Linked Dataspaces</a></td>
                  <td>Sarven Capadisli, 2012</td>
                </tr>
              </tbody>
            </table>
            <div class="issue">The table links to Designing URI Sets for the UK
              Public Sector. A newer version of this document (which was the
              first of its kind) exists but is on a <a href="https://github.com/UKGovLD/URI-patterns-core/blob/master/URI%20Patterns.md">GitHub
                repo</a>. It seems that this might happen due to changes in
              organisation behind data.gov.uk. If this happens, we should update
              the link to point to the new version.</div>
            <p>URIs can be long. In a dataset of even moderate size, storing
              each URI is likely to be repetitive and obviously wasteful.
              Instead, define locally unique identifiers for each element and
              provide data that allows them to be converted to globally unique
              URIs programmatically. The Metadata Vocabulary for Tabular Data
              [[tabular-metadata]] provides mechanisms for doing this within
              tabular data such as CSV files, in particular using <a href="http://www.w3.org/TR/tabular-metadata/#uri-template-properties">URI
                template properties</a> such as the <a href="http://www.w3.org/TR/tabular-metadata/#cell-aboutUrl">about
                URL</a> property.</p>
            <p>Where a data publisher is unable or unwilling to manage its URI
              space directly for persistence, an alternative approach is to use
              a redirection service such as <a href="http://purl.org/">purl.org</a>.
              This provides persistent URIs that can be redirected as required
              so that the eventual location can be ephemeral. The <a href="http://www.purlz.org/">software
                behind such services </a> is freely available so that it can be
              installed and managed locally if required.</p>
            <p>Digital Object Identifiers <a href="http://doi.org/">(DOIs)</a>
              offer a similar alternative. These identifiers are defined
              independently of any Web technology but can be appended to a 'URI
              stub.' DOIs are an important part of the digital infrastructure
              for research data and and libraries. </p>
            <aside class="example">
              <p>To be done. Will take examples from UK guidelines</p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>Check that each dataset in question is identified using a URI
              that has been assigned under a controlled process as set out in
              the previous section. Ideally, the relevant Web site includes a
              description of the process and a credible pledge of persistence
              should the publisher no longer be able to maintain the URI space
              themselves.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-UniqueIdentifier">R-UniqueIdentifier</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-Citable">R-Citable</a></p>
          </section>
        </div>
        <div class="practice">
          <p><span id="identifiersWithinDatasets" class="practicelab">Use
              persistent URIs as identifiers within datasets</span></p>
          <p class="practicedesc">Datasets should use and reuse other people's
            URIs as identifiers where possible.</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>The power of the Web lies in the <em>Network effect</em>. The
              first telephone only became useful when the second telephone meant
              there was someone to call; the third telephone made both of them
              more useful yet. Data becomes more valuable if it refers to other
              people's data about the same thing, the same place, the same
              concept, the same event, the same person, and so on. That means
              using the same identifiers across datasets and making sure that
              your identifiers can be referred to by other datasets. When those
              identifiers are HTTP URIs, they can be looked up and more data
              discovered.</p>
            <p>These ideas are at the heart of the <a href="http://www.w3.org/DesignIssues/LinkedData.html">5
                Stars of Linked Data</a> where one data point links to another,
              and of <a href="http://dret.github.io/webdata/">Hypermedia</a>
              where links may be to further data or to services (or more
              generally 'affordances') that act on or relate to the data in some
              way. Examples include a bug reporting mechanisms, processors, a
              visualization engine, a sensor, an actuator etc. In both Linked
              Data and Hypermedia, the emphasis is put on the ability for
              machines to traverse from one resource to another following links
              that express relationships.</p>
            <p>That's the Web of Data.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>That one data item can be related to others across the Web
              creating a global information space accessible to humans and
              machines alike.</p>
            <div class="note">
              <p> I changed data point for data item. Bernadette</p>
            </div>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>This is a topic in itself and a general document such as this can
              only include superficial detail.</p>
            <p>Developers know that very often the problem they're trying to
              solve will have already been solved by other people. In the same
              way, if you're looking for a set of identifiers for obvious things
              like countries, currencies, subjects, species, proteins, cities
              and regions, Nobel prize winners ‚Äì someone's done it already. The
              steps described for <a href="http://www.w3.org/TR/ld-bp/#how-to-find-existing-vocabularies">discovering
                existing vocabularies</a> [[LD-BP]] can readily be adapted.</p>
            <ul>
              <li>ensure URI sets you use are published by a trusted group or
                organization;</li>
              <li>ensure URI sets have permanent URIs.</li>
            </ul>
            <p>If you can't find an existing set of identifiers that meet your
              needs then you'll need to create your own, following the patterns
              for URI persistence so that others will add value to your data by
              linking to it.</p>
            <aside class="example">
              <p>To follow. Plan to use
                http://dbpedia.org/resource/ISO_3166-1:GR as the example. Store
                'GB' and 'GR' as codes in a table but use CSVW to declare that
                these can be turned into URIs like
                http://dbpedia.org/resource/ISO_3166-1:GR.</p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>Check that within the dataset, references to things that don't
              change or that change slowly, such as countries, regions,
              organizations and people, as referred to by URIs or by short
              identifiers that can be appended to a URI stub. Ideally the URIs
              should resolve, however, they have value as globally scoped
              variables whether they resolve or not.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-UniqueIdentifier">R-UniqueIdentifier</a></p>
          </section>
        </div>
        <!-- end of URIs in datasets BP -->
        <div class="practice">
          <p><span id="VersionIdentifiers" class="practicelab">Assign URIs to
              dataset versions and series</span></p>
          <p class="practicedesc">URIs should be assigned to individual versions
            of datasets as well as the overall series. </p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Like documents, many datasets fall into natural series or groups.
              For example:</p>
            <ul>
              <li>noon temperature readings in central London 1850 to the
                present day;</li>
              <li>today's noon temperature in London;</li>
              <li>the temperature in London at noon on 3rd June 2015.</li>
            </ul>
            <p>In different circumstances, it will be appropriate to refer
              separately to each of these examples (and many like them). </p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>It should be possible to refer to a specific version of a dataset
              and to concepts such as a 'dataset series' and 'the latest
              version.' </p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>The W3C provides a good example of how to do this. The
              (persistent) URI for this document is
              http://www.w3.org/TR/2015/WD-dwbp-20150224/. That identifier
              points to an immutable snapshot of the document on the day of its
              publication. The URI for the 'latest version' of this document is
              http://www.w3.org/TR/dwbp/ which is an identifier for a series of
              closely related documents that are subject to change over time. At
              the time of publication, these two URIs both resolve to this
              document. However, when the next version of this document is
              published, the 'latest version' URI will be changed to point to
              that. </p>
            <aside class="example">
              <p>To complete the London temperature example, one might imagine
                URIs as follows: </p>
              <pre class="highlight">  http://weather.example.com/temperature/UK/London/noon
  http://weather.example.com/temperature/UK/London/noon/today
  http://weather.example.com/temperature/UK/London/noon/2015-06-03  </pre>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p> Check that each version of a dataset has its own URI, and that
              logical groups of datasets are also identifiable. </p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-UniqueIdentifier">R-UniqueIdentifier</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-Citable">R-Citable</a></p>
          </section>
        </div>
        <!-- end of Data Identification BP --> </section>
      <!-- begin of Data Formats -->
      <section id="dataFormats">
        <h3>Data Formats</h3>
        <p>The formats in which data is made available to consumers are a key
          aspect of making that data usable. The best, most flexible access
          mechanism in the world is pointless unless it serves data in formats
          that enable use and reuse. Below we detail best practices in selecting
          formats for your data, both at the level of files and that of
          individual fields. W3C encourages use of formats that can be used by
          the widest possible audience and processed most readily by computing
          systems. Source formats, such as database dumps or spreadsheets, used
          to generate the final published format, are out of scope. This
          document is concerned with what is actually published rather than
          internal systems used to generate the published data.</p>
        <!-- begin of Machine-Readable Standardized Format BP -->
        <div class="practice">
          <p><span id="MachineReadableStandardizedFormat" class="practicelab">Use
              machine-readable standardized data formats </span></p>
          <p class="practicedesc">Data must be available in a machine-readable
            standardized data format that is adequate for its intended or
            potential use.</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>As data becomes more ubiquitous, and datasets become larger and
              more complex, processing by computers becomes ever more crucial.
              Posting data in a format that is not machine readable places
              severe limitations on the continuing usefulness of the data. Data
              becomes useful when it has been processed and transformed into
              information. </p>
            <p> Using non-standard data formats is costly and inefficient, and
              the data may lose meaning as it is transformed. On the other hand,
              standardized data formats enable interoperability as well as
              future uses, such as remixing or visualization, many of which
              cannot be anticipated when the data is first published. The use of
              non-proprietary data formats should also be considered since it
              increases the possibilities for use and re-use of data </p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p> It should be possible for machines to easily read and process
              data published on the Web. </p>
            <p> It should be possible for data consumers to use computational
              tools typically available in the relevant domain to work with the
              data. </p>
            <p> It should be possible for data consumers who wants to use or
              re-use the data to do so without investment in proprietary
              software.</p>
            <!-- Published data on the Web must be readable and easily processable by
              typical computing systems. Any data consumer who wishes to work              with the data and is authorized to do so must be able to do so              with computational tools typically available in the relevant              domain. -->
            <!--<p> A machine must be able to: </p>
            <ol>              <li>Open and read the data using commonly available software                packages. </li>              <li>Process the data. </li>              <li>Store the data for future use.</li>            </ol> -->
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p> Make data available in a machine readable standardized data
              format that is easily parseable including but not limited to CSV,
              XML, Turtle, NetCDF, JSON and RDF. </p>
            <!--<p>Consider which data formats potential users of the data are most
              likely to have the necessary tools to parse. Formats suggestion              are shown in the <a href="#OpenFormat"></a>.               Standard data              formats as well as the use of standard data vocabularies will              better enable machines to process the data.-->
            <aside class="example">
              <p> To provide data about the bus timetables John chooses the <a

                  href="http://www.w3.org/TR/tabular-data-model/"> tabular
                  format </a> and creates a new distribution for dataset-001.
                To promote interoperability and to help the automatic data
                processing, John adopts the GTFS standard to describe the data
                about the bus time tables. According to GTFS John needs to
                create several csv files, which describe information about bus
                stops, routes, trips, stop times, bus frequencies as well as
                information about the starting and ending time of the bus
                service during the week, the weekend and holidays. As described
                in the tabular data model, John creates a group of tables to
                connect the several csv files used to describe the bus
                timetables. In this case, the csv distribution (dataset-001-csv)
                of the dataset-001 corresponds to a group of tables as described
                below.</p>
              <pre class="highlight"> 
   :dataset-001
       a dcat:Dataset;
       dcat:distribution :dataset-001-csv;
       .
   :dataset-001-csv
       a dcat:Distribution;
       dcat:downloadURL &lt;http://example.org/bus_timetables_mycity.json&gt;
       dct:title "CSV distribution of the bus timetable dataset of MyCity."
       dcat:mediaType "text/csv";
       dct:license &lt;http://creativecommons.org/licenses/by-sa/3.0/&gt;;
       .

    http://example.org/bus_timetables_mycity.json
    {
      "@context": "http://www.w3.org/ns/csvw",
      "tables": [{
          "url": "https://example.org/stops.csv",
          "tableSchema": "https://example.org/stops.json"
      }, {
          "url": "https://example.org/routes.csv",
          "tableSchema": "https://example.org/routes.json"
      }, {
          "url": "https://example.org/trips.csv",
          "tableSchema": "https://example.org/trips.json"
      }, {
          "url": "https://example.org/stop_times.csv",
          "tableSchema": "https://example.org/stop_times.json"
      }, {
          "url": "https://example.org/frequencies.csv",
          "tableSchema": "https://example.org/frequencies.json"
      }, {
          "url": "https://example.org/calendar.csv",
          "tableSchema": "https://example.org/calendar.json"
      }, {
          "url": "https://example.org/calendar_dates.csv",
          "tableSchema": "https://example.org/calendar_dates.json"
      }]
    }
</pre> </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p> Check that the data format conforms to a known machine-readable
              data format specification. </p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-FormatMachineRead">R-FormatMachineRead</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-FormatStandardized">R-FormatStandardized</a>
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-FormatOpen">R-FormatOpen</a></p>
          </section>
        </div>
        <!-- end of Machine-Readable Standardized FormatBP -->
        <!-- begin of Open Format BP 
        <div class="practice">          <p><span id="OpenFormat" class="practicelab">Use non-proprietary data formats </span></p>          <p class="practicedesc">Data should be            available in a nonproprietary data format. </p>          <section class="axioms">            <p class="subhead">Why</p>            <p> Non-proprietary data formats are usable by anyone. Proprietary              data formats may be difficult or impractical for some data users              to view or parse. Thus, the use of open data formats increases the              possibilities for use and re-use of data.</p>          </section>          <section class="outcome">            <p class="subhead">Intended Outcome</p>            <p>It should be possible for any person who wants to use or re-use              the data to do so without investment in proprietary software.</p>          </section>          <section class="how">            <p class="subhead">Possible Approach to Implementation</p>            <p> Make data available in open data formats including but not              limited to CSV, XML, Turtle, NetCDF, JSON and RDF.</p>            <label class="expand" for="example-NonproprietaryDataFormat"></label>            <input id="example-NonproprietaryDataFormat" type="checkbox">            <div class="example">              <p>to be done </p>            </div>          </section>          <section class="test">            <p class="subhead">How to Test</p>            <p> Check if it is possible to read, process, and store the data              without using any proprietary software package. </p>          </section>          <section class="ucr">            <p class="subhead">Evidence</p>            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-FormatOpen">R-FormatOpen</a></p>          </section>        </div>-->
        <!-- end of Open Formats BP -->
        <!-- begin of Multiple Formats BP -->
        <div class="practice">
          <p><span id="MultipleFormats" class="practicelab">Provide data in
              multiple formats </span></p>
          <p class="practicedesc">Data should be available in multiple data
            formats. </p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Providing data in more than one format reduces costs incurred in
              data transformation. It also minimizes the possibility of
              introducing errors in the process of transformation. If many users
              need to transform the data into a specific data format, publishing
              the data in that format from the beginning saves time and money
              and prevents errors many times over. Lastly it increases the
              number of tools and applications that can process the data.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p> It should be possible for data consumers to work with the data
              without transforming it.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p> Consider the data formats most likely to be needed by intended
              users, and consider alternatives that are likely to be useful in
              the future. Data publishers must balance the effort required to
              make the data available in many formats, but providing at least
              one alternative will greatly increase the usability of the data.</p>
            <aside class="example">
              <p> In order to reach a larger number of data consumers John
                decides to provide RDF and XML distributions for the
                dataset-001. </p>
              <pre class="highlight"> 
  :dataset-001
       a dcat:Dataset;
       dcat:distribution :dataset-001-csv;
       dcat:distribution :dataset-001-rdf;
       dcat:distribution :dataset-001-xml;
       .
  :dataset-001-csv
       a dcat:Distribution;
       dcat:downloadURL &lt;http://example.org/bus_timetables_mycity.json&gt;
       dct:title "CSV distribution of the bus timetable dataset of MyCity."
       dcat:mediaType "text/csv";
       dct:license &lt;http://creativecommons.org/licenses/by-sa/3.0/&gt;;
       .
  :dataset-001-rdf
       dcat:downloadURL &lt;http://example.org/bus_timetables_mycity.rdf&gt;
       dct:title "RDF distribution of the bus timetable dataset of MyCity."
       dcat:mediaType "text/rdf";
       dct:license &lt;http://creativecommons.org/licenses/by-sa/3.0/&gt;;
       .
  :dataset-001-xml
       a dcat:Distribution;
       dcat:downloadURL &lt;http://example.org/bus_timetables_mycity.xml&gt;
       dct:title "XML distribution of the bus timetable dataset of MyCity."
       dcat:mediaType "text/xml";
       dct:license &lt;http://creativecommons.org/licenses/by-sa/3.0/&gt;;
       .   
            </pre>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p> Check that the complete dataset is available in more than one
              data format.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-FormatMultiple">R-FormatMultiple</a></p>
          </section>
        </div>
        <!-- end of Multiple Formats BP --> </section>
      <!-- end of Data Formats -->
      <!-- begin of Data Vocabularies -->
      <section id="dataVocabularies">
        <h3>Data Vocabularies</h3>
        <!--<div class="issue"> There is a discussion going on in the group if the
          creation (and publication) of vocabularies is in the scope of the DWBP          document.</div>-->
        <p>Data is often represented in a structured and controlled way, making
          reference to a range of vocabularies, for example, by defining types
          of nodes and links in a data graph or types of values for columns in a
          table, such as the subject of a book, or a relationship ‚Äúknows‚Äù
          between two persons. Additionally, the values used may come from a
          limited set of pre-existing values or resources: for example object
          types, roles of a person, countries in a geographic area, or possible
          subjects for books. Such vocabularies ensure a level of control,
          standardization and interoperability in the data. They can also serve
          to improve the usability of datasets. Say, a dataset contains a
          reference to a concept described in several languages. Such reference
          allows applications to localize their display of their search
          depending on the language of the user. </p>
        <p>According to W3C, <a href="http://www.w3.org/standards/semanticweb/ontology">vocabularies</a>
          define the concepts and relationships (also referred to as ‚Äúterms‚Äù or
          ‚Äúattributes‚Äù) used to describe and represent an area of concern.
          Vocabularies are used to classify the terms that can be used in a
          particular application, characterize possible relationships, and
          define possible constraints on using those terms. Several categories
          of vocabularies have been coined, for example, ontology, controlled
          vocabulary, thesaurus, taxonomy, code list, semantic network.</p>
        <p>There is no strict division between the artifacts referred to by
          these names. ‚ÄúOntology‚Äù tends however to denote the vocabularies of
          classes and properties that structure the descriptions of resources in
          (linked) datasets. In relational databases, these correspond to the
          names of tables and columns; in XML, they correspond to the elements
          defined by an XML Schema. Ontologies are the key building blocks for
          inference techniques on the Semantic Web. The first means offered by
          W3C for creating ontologies is the RDF Schema [[RDF-SCHEMA]] language.
          It is possible to define more expressive ontologies with additional
          axioms using languages such as those in The Web Ontology Language
          [[OWL2-OVERVIEW]]. </p>
        <p>On the other hand, ‚Äúcontrolled vocabularies‚Äù, ‚Äúconcept schemes‚Äù,
          ‚Äúknowledge organization systems‚Äù enumerate and define resources that
          can be employed in the descriptions made with the former kind of
          vocabulary. A concept from a thesaurus, say, ‚Äúarchitecture‚Äù, will for
          example be used in the subject field for a book description (where
          ‚Äúsubject‚Äù has been defined in an ontology for books). For defining the
          terms in these vocabularies, complex formalisms are most often not
          needed. Simpler models have thus been proposed to represent and
          exchange them, such as the ISO 25964 data model [[ISO-25964]] or W3C's
          Simple Knowledge Organization System [[SKOS-PRIMER]].</p>
        <!-- <p>This section presents best practices for data vocabularies accessible
          as URI sets on the Web, which are applicable to any kind of          vocabulary.</p> -->
        <!-- begin of Document Vocabularies BP -->
        <!-- begin of Provide Metadata Standardized BP -->
        <div class="practice">
          <p><span id="MetadataStandardized" class="practicelab">Use
              standardized terms</span></p>
          <p class="practicedesc"> Standardized terms should be used to provide
            data and metadata</p>
          <div class="issue"> A definition for standardized term should be
            included in the glossary to clarify the difference between
            standardized terms and vocabulary. <a href="https://www.w3.org/2013/dwbp/track/issues/209">
              Issue-209 </a> </div>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>The need for code lists and other commonly used terms for data
              values and for describing metadata is to avoid as much as possible
              ambiguity and clashes in the terms chosen for data and metadata
              information. The key reason is to be able to refer to the
              standardized body/organization which defines the term or code as a
              clear reference.</p>
            <!--<div class="issue"> Code lists are used for describing metadata or
              data values? If code lists are used for describing data values              then it would be nice to have a separate BP for this. The title of
              this BP mentions just metadata. <a href="https://www.w3.org/2013/dwbp/track/issues/210">                Issue-210 </a> </div> -->
            <section class="outcome">
              <p class="subhead">Intended Outcome</p>
              <p>The benefit of using standardized code lists and other commonly
                used terms is to enable interoperability and consensus among
                data publishers and consumers. </p>
            </section>
            <section class="how">
              <p class="subhead">Possible Approach to Implementation</p>
              <p>An approach to implementation is the case of a vocabulary
                developed within a Working Group or a standardized body such as
                the W3C.</p>
              <p>The Open Geospatial Consortium (OGC) could define the notion of
                <em>granularity</em> for geospatial datasets, while [DCAT]
                vocabulary provides a vocabulary reusing the same notion applied
                to catalogs on the Web.</p>
              <aside class="example">
                <pre>to be done</pre>
              </aside>
            </section>
            <section class="test"><span style="font-weight: bold;">How to Test </span>
              <p>Check that the terms or codes to be used are defined in a
                standard organization/working group of body such as IETF, OGC,
                W3C, etc.</p>
              <section class="ucr">
                <p class="subhead">Evidence</p>
                <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataStandardized">R-MetadataStandardized</a>,
                  <a href="http://www.w3.org/TR/dwbp-ucr/#R-QualityComparable">R-QualityComparable</a></p>
              </section>
            </section>
          </section>
        </div>
        <div class="practice">
          <p><span id="ReuseVocabularies" class="practicelab">Re-use
              vocabularies</span></p>
          <p class="practicedesc"> Shared vocabularies should be used to provide
            metadata</p>
          <!-- <div class="issue"> As proposed during the F2F, it would be nice to
            use the term shared vocabularies, i.e., vocabularies that capture a            consensus of the community about a specific domain, instead of
            vocabularies. In the context of this BP, I think it is more            appropriate to say "Use shared vocabularies" instead of "Reuse
            vocabularies". I think Reuse of vocabularies is more appropriate            when we are talking about the creation of new vocabularies . <a href="https://www.w3.org/2013/dwbp/track/issues/212">
              Issue-212 </a></div> -->
          <section class="axioms">
            <p class="subhead">Why</p>
            <p> Re-using vocabularies increases interoperability and reduces
              redundancies, encouraging re-use of the data. Shared vocabularies
              capture a consensus of the community about a specific domain. The
              re-use of shared vocabularies to describe metadata helps the
              automatic processing of data and metadata. Shared vocabularies
              should be especially used to describe both structural metadata as
              well as other types of metadata (descriptive, provenance, quality
              and versioning). </p>
            <!-- <div class="issue"> I propose to change the Why section to be more
              specific about the use of shared vocabularies. <a href="https://www.w3.org/2013/dwbp/track/issues/211">                Issue-211 </a> </div> -->
            <section class="outcome">
              <p class="subhead">Intended Outcome</p>
              <p> It should be possible to automatically compare two or more
                datasets when they use the same vocabulary to describe metadata.
              </p>
              <p> It should be possible for machines to automatically process
                the data within a dataset. </p>
              <p> It should be possible for machines to automatically process
                the metadata that describes a dataset. </p>
            </section>
            <section class="how">
              <p class="subhead">Possible Approach to Implementation</p>
              <p> The <a href="http://www.w3.org/TR/ld-bp/#VOCABULARIES">Standard
                  Vocabularies</a> section of the W3C Best Practices for
                Publishing Linked Data [[LD-BP]] provides guidance on the
                discovery, evaluation and selection of existing vocabularies.</p>
              <aside class="example">
                <pre>to be done</pre>
              </aside>
            </section>
            <section class="test"><span style="font-weight: bold;">How to Test </span>
              <p>Check that terms or attributes used do not replicate those
                defined by vocabularies in common use within the same domain.</p>
              <section class="ucr">
                <p class="subhead">Evidence</p>
                <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataStandardized">R-MetadataStandardized</a>,
                  <a href="http://www.w3.org/TR/dwbp-ucr/#R-VocabReference">R-VocabReference</a>
                </p>
              </section>
            </section>
          </section>
        </div>
        <!-- begin of Not Over formalize Vocabularies BP -->
        <div class="practice">
          <p><span id="ChoseRightFormalizationLevel" class="practicelab">Choose
              the right formalization level</span></p>
          <p class="practicedesc">When reusing a vocabulary, a data publisher
            should opt for a level of formal semantics that fit data and
            applications.</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Formal semantics may help one to establish precise specifications
              that support establishing the intended meaning of the vocabulary
              and the performance of complex tasks such as reasoning. On the
              other hand, complex vocabularies require more effort to produce
              and understand, which could hamper their re-use, as well as the
              comparison and linking of datasets exploiting them. Highly
              formalized data is also harder to exploit by inference engines:
              for example, using an OWL class in a position where a SKOS concept
              is enough, or using OWL classes with complex OWL axioms raises the
              formal complexity of the data according to the OWL Profiles
              [[OWL2-PROFILES]]. Data producers should therefore seek to
              identify the right level of formalization for particular domains,
              audiences and tasks, and maybe offer different formalization
              levels when one size does not fit all.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>The data supports all application cases but should not be more
              complex to produce and re-use than necessary;</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>Identify the "role" played by the vocabulary for the datasets,
              say, providing classes and properties used to type resources and
              provide the predicates for RDF statements, or elements in an XML
              Schema, as opposed to providing simple concepts or codes that are
              used for representing attributes of the resources described in a
              dataset. When simpler models are enough to convey the necessary
              semantics, represent vocabularies using them. For instance, for
              Linked Data, SKOS may be preferred for simple vocabularies as
              opposed to formal ontology languages like OWL; see for example how
              <a href="http://www.w3.org/TR/vocab-data-cube/#schemes">concept
                schemes and code lists</a> are used in the RDF Data Cube
              Recommendation [[QB]].</p>
            <aside class="example">
              <pre>to be done</pre>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>For formal knowledge representation languages, applying an
              inference engine on top of the data that uses a given vocabulary
              does not produce too many statements that are unnecessary for
              target applications.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-VocabReference">R-VocabReference</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-VocabDocum">R-VocabDocum</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-QualityComparable">R-QualityComparable</a>
            </p>
          </section>
        </div>
        <div class="issue">The best practice on formalization above (especially
          sections "Intended outcome" and "How to test") should be re-written in
          a more technology-neutral way. <a href="https://www.w3.org/2013/dwbp/track/issues/144">Issue-144</a></div>
        <!-- end of Not Over formalize Vocabularies BP -->
        <!--        <h3>How to choose vocabularies?</h3>
        <p> It is very important to know how to choose the vocabulary to be          re-used. For doing this, see <a href="http://www.w3.org/TR/ld-bp/#VOCABULARIES">Vocabulary            Checklist</a> </p>        <div class="issue"> The <a href="http://www.w3.org/TR/ld-bp/#VOCABULARIES">Vocabulary            Checklist</a> from the Best Practices for Publishing Linked Data          overlaps quite a lot with the best practices from this section. Should          we clarify the difference in positioning? Is there one?          <a href="https://www.w3.org/2013/dwbp/track/issues/124">              Issue-124</a></p>        </div> -->
      </section>
      <!-- end of Data Vocabularies -->
      <!-- begin of Data Granularity -->
      <!-- 
      <section id="granularity">        <h3>Data Granularity</h3>        <p>The section on granularity</p>              </section>      -->
      <!-- end of Data Granularity -->
      <!-- begin of Sensitive Data -->
      <section id="sensitive">
        <h3>Sensitive Data</h3>
        <p>To support best practices for publishing sensitive data, data
          publishers should identify all sensitive data, assess the exposure
          risk, determine the intended usage, data user audience and any related
          usage policies, obtain appropriate approval, and determine the
          appropriate security measures needed to taken to protect the data,
          which should also account for secure authentication and use of HTTPS.</p>
        <p>Data publishers should preserve the privacy of individuals where the
          release of personal information would endanger safety (unintended
          accidents) or security (deliberate attack). Privacy information might
          include: full name, home address, mail address, national
          identification number, IP address (in some cases), vehicle
          registration plate number, driver's license number, face,
          fingerprints, or handwriting, credit card numbers, digital identity,
          date of birth, birthplace, genetic information, telephone number,
          login name, screen name, nickname, health records etc.</p>
        <p> At times, because of sharing policies sensitive data may not be
          available in part or in its entirety. Data unavailability represents
          gaps that may affect the overall analysis of datasets. To account for
          unavailable data, data publishers should publish information about
          unavoidable data gaps.</p>
        <div class="practice">
          <p><span id="DataUnavailabilityReference" class="practicelab">Provide
              data unavailability reference</span></p>
          <p class="practicedesc">References to data that is not open, or
            available under different restrictions to the origin of the
            reference, should provide explanation about how the referred data
            can be accessed and who can access it. </p>
          <!-- <p> This best practice is a specialization of the higher level <a href="#ProvideMetadataHumanMachine">Provide
              metadata for both humans and machines</a> best practice. </p> -->
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Publishing online documentation about unavailable data due to
              sensitivity issues provides a means for publishers to explicitly
              identify knowledge gaps. This provides a contextual explanation
              for consumer communities thus encouraging use of the data that <em>is</em>
              available.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>Publishers should provide information about data that is referred
              to from the current dataset but that is unavailable or only
              available under different conditions.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>Data publishers may publish an HTML document that gives a
              human-readable explanation for data unavailability. RDF may be
              used to provide a machine readable version of the same
              information. If appropriate, consider editing the server's 4xx
              response page(s) to provide the information.</p>
            <aside class="example">
              <p>The datasets created for the bus timetables contain sensitive
                data, for instance privacy information about the bus` driver, so
                it has a description informing that the personal data about the
                bus` driver is not available.</p>
              <p>The information bellow was omitted from the dataset:</p>
              <p>driver_id, driver_name, driver_address, driver_license.</p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>If the dataset includes references to other data that is
              unavailable, check whether an explanation is available in the
              metadata and/or description of it.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-AccessLevel">R-AccessLevel</a></p>
          </section>
        </div>
        <!-- end of BP --> </section>
      <!-- end of Data Sensitive -->
      <!-- begin of Data Access -->
      <section id="dataAccess">
        <h3>Data Access</h3>
        <p>Providing easy access to data on the Web enables both humans and
          machines to take advantage of the benefits of sharing data using the
          Web infrastructure. By default, the Web offers access using Hypertext
          Transfer Protocol (HTTP) methods. This provides access to data at an
          atomic transaction level. However, when data is distributed across
          multiple files or requires more sophisticated retrieval methods
          different approaches can be adopted to enable data access, including
          bulk download and <abbr title="Application Programming Interfaces">API</abbr>s.
          </p>
        <p>One approach is packaging data in bulk using non-proprietary file
          formats (for example tar files). Using this approach, bulk data is
          generally pre-processed server side where multiple files or directory
          trees of files are provided as one downloadable file. When bulk data
          is being retrieved from non-file system solutions, depending on the
          data user communities, the data publisher can offer APIs to support a
          series of retrieval operations representing a single transaction.</p>
        <p>For data that is streaming to the Web in ‚Äúreal time‚Äù or ‚Äúnear real
          time‚Äù, data publishers should publish data or use APIs to enable
          immediate access to data, allowing access to critical time sensitive
          data, such as emergency information, weather forecasting data, or
          published system metrics. In general, APIs should be available to
          allow third parties to automatically search and retrieve data
          published on the Web. </p>
        <p>On a further note, it can be observed that data on the Web is
          essentially about the description of entities identified by a unique,
          Web-based, identifier (an URI). Once the data is dumped and sent to an
          institute specialised in digital preservation the link with the Web is
          broken (dereferencing) but the role of the URI as a unique identifier
          still remains. In order to increase the usability of preserved dataset
          dumps it is relevant to maintain a list of these identifiers. </p>
        <!-- begin of BP Bulk Access-->
        <div class="practice">
          <p><span id="BulkAccess" class="practicelab">Provide bulk download </span></p>
          <p class="practicedesc">Data should be available for bulk download. </p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>When web data is distributed across many URLs and logically
              organized as one container, accessing the data in bulk is useful.
              Bulk access provides a consistent means to handle the data as one
              container. Without it, individually accessing data is cumbersome
              leading to inconsistent approaches to handling the container.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p> It should be possible to download data on the Web in bulk. Data
              publishers should provide a way either through bulk file formats
              or APIs for consumers to access this type of data.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>Depending on the nature of the data and consumer needs possible
              approaches could include:</p>
            <ul>
              <li>Preprocessing a copy of the data in compressed archive format
                where the data more easily accessible as one URL. This is
                particularly useful for handling data that changes infrequently
                or on a scheduled basis.</li>
              <li>Hosting an API such as a REST or SOAP service that dynamically
                retrieves individual data and returns a bulk container. This
                approach is useful when for capturing a snapshot of the data.
                The API can also be useful for consumers to customize what they
                want included or excluded. </li>
              <li>Hosting a database, web page, or SPARQL endpoint that contains
                discoverable metadata [[VOCAB-DCAT]] describing the container
                and data URLs associated with the container.</li>
            </ul>
            <aside class="example">
              <p>to be done </p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>Humans can retreive copies of preprocessed bulk data through
              existing tools such as a browser. Clients can test bulk access
              through an API or queries to web resources with discoverable
              metadata about the bulk data. </p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-AccessBulk">R-AccessBulk</a></p>
          </section>
        </div>
        <!-- end of BP Bulk Access -->
        <!-- begin of BP Bulk Access 2-->
        <div class="practice">
          <p><span id="DesigningAPIs" class="practicelab">Follow REST principles
              when designing APIs</span></p>
          <p class="practicedesc">APIs for accessing data should follow <abbr title="Representational State Transfer">REST</abbr>
            architectural approaches.</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Considering RESTful architectural aspects when designing an APIs
              can guarantee easier development, use of pre-existing
              infrastructure (the Web), a shorter learning curve for developers
              that want to build applications that access data. It also assures
              sustainability as "the technologies that make up this foundation
              include the Hypertext Transfer Protocol (HTTP), Uniform Resource
              Identifier (URI), markup languages such as HTML and XML, and
              Web-friendly formats" [[RICHARDSON]]. Furthermore, it can mitigate
              the use of specific clients or the need of <abbr title="Universal Description, Discovery and Integration">UDDI</abbr>.</p>
            <p>APIs are frequently constructed over different approaches, such
              as SOAP. For data on the Web context, the architecture of the Web
              itself described at the documentation of REST architectural style
              offers the same entry for humans and machines to access data. If
              humans already have access to data in URLs, it can be also
              structured for offering multiple representations for formats and
              use content negotiation between applications easily.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <ul>
              <li>It should be possible for machines to access data in a variety
                of formats from the same URI through content negotiation.</li>
              <li> It should be possible for data consumers to access data using
                browser as a client. </li>
            </ul>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>Design always RESTful APIs using HTTP and good pragmatic REST
              principles. There is no unique agreed set of principles for REST
              APIs, some are implicitly defined by the HTTP standard and others
              have emerged on a consensus base or even are still under
              discussion. The following are a set of rules widely adopted so
              far:</p>
            <ul>
              <li>Use hierarchical, readable and technology agnostic Uniform
                Resource Identifiers (URIs) to address resources in a consisten
                way.</li>
              <li>Use the URI path to convey your Resources and Collections
                model.</li>
              <li>Use nouns but no verbs (except for Controllers that does not
                involve resources). Use HTTP verbs instead to operate on the
                Collections and Resources.</li>
              <li>Use standard HTTP methods accordingly to their expected
                default behavior. GET method and query parameters should not
                alter the state.</li>
              <li>Use HTTP headers to provide metadata and for the serialization
                of data formats. Support multiple formats.</li>
              <li>Use HTTP status codes (including error codes) accordingly to
                their original purpose.</li>
              <li>Simplify associations. Use query parameters to hide complexity
                and provide filtering, sorting, field selection and paging for
                collections.</li>
              <li>Version your API. Never release an API without a version and
                make the version mandatory.</li>
            </ul>
            <aside class="example">
              <p>to be done </p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>Use API testing tools to compare benefits of implementing RESTful
              design.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-AccessBulk">R-AccessBulk</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-APIDocumented">R-APIDocumented</a></p>
          </section>
        </div>
        <!-- end of BP BulkAccess 2-->
        <!-- begin of BP Content Negotiation -->
        <div class="practice">
          <p><span id="Conneg" class="practicelab">Serving data and resources
              with different formats </span></p>
          <p class="practicedesc">It is recommended to use content negotiation
            for serving data available in multiple formats</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>It is possible to have data being served in a HTML page mixed
              with human-readable and machine-readable data. RDFa could be used
              to mix HTML content with semantic data. </p>
            <p>But, in some cases this page is subject of scraping by some
              applications in order to get data available. When structured data
              is mixed with HTML, but it is possible to have a different
              representation with the same structured data, written in Turtle or
              JSON-LD, it is recommended to serve this page using Content
              Negotiation. </p>
            <div class="issue">
              <p>This BP will be complemented.</p>
            </div>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p> It should be possible to serve the same resource with different
              representations. </p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>A possible approach to implementation is to configure the web
              server to deal with content negotiation of the requested resource.</p>
            <ul>
              <li>http://example.org/profile_info.html - Personal information
                represented in HTML + RDFa</li>
              <li>http://example.org/profile_info.json - The same information of
                the resource but represented in JSON-LD format</li>
              <li>http://example.org/profile_info.ttl - The same information of
                the resource but represented in Turtle format</li>
            </ul>
            <p>The specif format of the resource's representation can be acessed
              by the URI or by the Content-type of the HTTP Request.</p>
            <aside class="example">
              <p>to be done </p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: </p>
          </section>
        </div>
        <!-- end of BP Content Negotiation -->
        <!-- begin of BP Access Real-time-->
        <div class="practice">
          <p><span id="AccessRealTime" class="practicelab">Provide real-time
              access </span></p>
          <p class="practicedesc">When data is produced in real-time, it should
            be available on the Web in real-time. </p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p> The presence of real-time data on the web enables access to
              critical time sensitive data, and encourages the development of
              real-time web applications. Real-time access is dependent on
              real-time data producers making their data readily available to
              the data publisher. The necessity of providing real-time access
              for a given application will need to be evaluated on a case by
              case basis considering refresh rates, latency introduced by data
              post processing steps, infrastructure availability, and the data
              needed by consumers. In addition to making data accessible, data
              publishers may provide additional information describing data
              gaps, data errors and anomolies, and publication delays.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>Data should be available at real time or near real time, where
              real-time means a range from milliseconds to a few seconds after
              the data creation, and near real time is a predetermined delay for
              expected data delivery. </p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>Real-time data accessibility may be achieved through two means: </p>
            <ul>
              <li> Push - as data is produced the producers communicates data to
                the data publisher either by disseminating data to the publisher
                or making storage available accessible to the data producer.</li>
              <li>On-Demand (Pull) - available real-time data is made available
                upon request. In this case, data publishers will provide an API
                to facilitate these read-only requests.</li>
            </ul>
            In addition to data access, to ensure credibility providing access
            to error conditions, anomolies, and instrument "house keeping" data
            enhance real-time applications ability to interpret and convey
            real-time data quality to consumers.
            <aside class="example">
              <p>to be done </p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            To adequately test real time data access, data will need to be
            tracked from the time it is initially collected to the time it is
            published and accessed. [[PROV-O]] can be used to describe these
            activities. Caution should be used when analyzing real-time access
            for systems that consist of multiple computer systems. For example,
            tests that rely on wall clock time stamps may reflect inconsistences
            between the individual computer systems as opposed to data
            publication time latency. </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-AccessRealTime">R-AccessRealTime</a></p>
          </section>
        </div>
        <!-- end of BP Access Real-time-->
        <!-- begin of BP Access Up to date -->
        <div class="practice">
          <p><span id="AccessUptoDate" class="practicelab">Provide data up to
              date </span></p>
          <p class="practicedesc"> Data must be available in an up-to-date
            manner and the update frequency made explicit. </p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Data on the Web availability should closely coincide with data
              provided at creation time, collection time, or after it has been
              processed or changed. Carefully synchronizing data publication to
              the update frequency encourages data consumer confidence and
              re-use.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>When new data is provided or data is updated, it must be
              published to coincide with the data changes.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>Implement an API to enable data access. When data is provided by
              bulk access, new files with new data should be provided as soon as
              additional data is created or updated. </p>
            <aside class="example">
              <p>to be done </p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>Write test standard operating procedure for data publisher to
              keep test data on Web site up to date.</p>
            <p> Following standard operating procedure:</p>
            <ul>
              <li> Write test client to access published data. </li>
              <li> Access data and save first copy locally. </li>
              <li> Publish an updated version of data.</li>
              <li> Access data and save second copy locally.</li>
              <li> Compare first copy to second copy to verify change.</li>
            </ul>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-AccessUptodate">R-AccessUptodate</a></p>
          </section>
        </div>
        <div class="issue">
          <p> To debate if the goal should be to adhere to a published schedule
            for updates. <a href="http://www.w3.org/2013/dwbp/track/issues/195">Issue-195</a></p>
        </div>
        <!-- begin of versions For API BP -->
        <div class="practice">
          <p><span id="VersionsForAPI" class="practicelab">Maintain separate
              versions for a data API</span></p>
          <p class="practicedesc">If data is made available through an API, the
            API itself should be versioned separately from the data. Old
            versions should continue to be available.</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Developers need to be made aware of changes to an API so that
              they can update their code to use it. When an API is changed, as
              opposed to when the data it makes available is changed, releasing
              it as a new version makes it possible to gracefully transition
              from the old version to the new one. Keeping the older versions
              available avoids breaking applications that cannot be updated.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>It should be possible for developers to transition easily from
              one version of the API to another. Applications that are
              impractical to transition should continue to work. The API version
              should not be updated when data versions are updated, only when
              the API itself changes, and that should be infrequent.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>Release updates to your API under a slightly different base URI
              so that older versions remain available under the previous base
              URI. For example, <a href="http://myapi.org/v1/dogs/alfred">http://myapi.org/v1/dogs/alfred</a>
              retrieves the older version of data about a dog named Alfred, and
              <a href="http://myapi.org/v2/dogs/alfred">http://myapi.org/v2/dogs/alfred</a>
              retrieves the newer version of data about Alfred. Keeping the
              version number as far to the left as possible in the API call
              allows developers to switch to the newer version with the least
              effort.</p>
            <div class="example">
              <p>to be done </p>
            </div>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>Existing calls to the API should continue to work when the API is
              updated. New calls to a slightly different base URI should
              retrieve data according to the new rules.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-DataVersion">R-DataVersion</a></p>
          </section>
        </div>
        <!-- end of versions For API BP --> </section>
      <!-- end of BP Access Up to date-->
      <!-- end of Data Access -->
      <!-- begin of Data Preservation -->
      <section id="dataPreservation">
        <h3>Data Preservation</h3>
        This section describes best practices related to <a href="http://w3c.github.io/dwbp/glossary.html#data-preservation-1">data
          preservation</a>. Albeit being a closely related topic <a href="http://w3c.github.io/dwbp/glossary.html#data-archiving-1data">archiving</a>
        is considered out of scope for this group and therefore not covered
        here.
        <!--
        <p>Data preservation is a well understood and commonly performed task          for static and self-contained data. This commonly includes the          following steps:</p>        <ul>          <li>ingest the data and assign a persistent identifier to it;</li>          <li>ensure the data is correctly stored and prevent bit rot;</li>          <li>provide access to the data and perform format translation if            needed.</li>        </ul>        <p>The model most commonly referred to is the Open Archival Information          System [[OAIS]]. Many digital preservation institutions are          implementing this model or some variant of it. Web pages can be          preserved following the same strategies, considering a Web site as a          static data set that is self-contained and can be preserved as a snap          shot at a fixed time. When preserving data on the Web some new          elements have to be taken into account:</p>        <ul>          <li>the persistent re-use (URIs) used across the Web may be related to            live data that can change;</li>          <li>the meaning of a resource is contextualized by the other resources            it is linked to;</li>          <li>documents fetched in HTML, RDF/XML of JSON, for instance, are only            one of the many possible serializations of the data they represent.</li>        </ul>        <p>The preservation of Web data should generally focus on the          preservation of the description of entities.</p>        -->
        <!-- begin of list of resources BP -->
        <!--
        <div class="practice">          <p><span id="list_resources" class="practicelab">Maintain a list of              resources described in a dataset</span></p>          <p class="practicedesc">A dataset should be            preserved together with a list of all the resources it describes</p>          <section class="axioms">            <p class="subhead">Why</p>            <p>data on the Web is essentially about the description of entities              identified by a unique, Web-based, identifier (a URI). Once the              data is dumped and sent to an institute specialised in digital              preservation the link with the Web is broken (de-referencing) but              the role of the URI as a unique identifier still remains. In order              to increase the usability of preserved dataset dumps it is              relevant to maintain a list of these identifiers.</p>          </section>          <section class="outcome">            <p class="subhead">Intended Outcome</p>            <p>It should be possible to look for a              preserved dataset based on the resources contained in it.</p>          </section>          <section class="how">            <p class="subhead">Possible Approach to Implementation</p>            <p>The list of resources can be created by the data depositor or the              digital repository at ingestion time. A dataset dump is scanned              for all the subject described and this list is stored separately.              For RDF and JSON-LD, this corresponds to all the resources in the              "subject" position of statements.</p>          </section>          <section class="test">            <p class="subhead">How to Test</p>            <p>Every resource listed in the resource list must              have some kind of description in the dataset</p>          </section>          <section class="ucr">            <p class="subhead">Evidence</p>            <p><span>Relevant requirements</span>:<a href="http://www.w3.org/TR/dwbp-ucr/#R-UniqueIdentifier">R-UniqueIdentifier</a></p>          </section>        </div>-->
        <!-- end of list of resources BP -->
        <!-- begin of assess dataset BP -->
        <div class="practice">
          <p><span id="EvaluateCoverage" class="practicelab">Assess dataset
              coverage</span></p>
          <p class="practicedesc">The coverage of a dataset should be assessed
            prior to its preservation</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>A chunk of Web data is by definition dependent on the rest of the
              global graph. This global context influences the meaning of the
              description of the resources found in the dataset. Ideally, the
              preservation of a particular dataset would involve preserving all
              its context. That is the entire Web of Data. </p>
            <p>At ingestion time an evaluation of the linkage of Web data
              dataset dump to already preserved resources is assessed. The
              presence of all the vocabularies and target resources in uses is
              sought in a set of digital archives taking care of preserving Web
              data. Datasets for which very few of the vocabularies used and/or
              resources pointed out are already preserved somewhere should be
              flagged as being at risk.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>It should be possible to appreciate the coverage and external
              dependencies of a given dataset.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>The assessement can be performed by the digital preservation
              institute or the dataset depositor. It essentially consists in
              checking whether all the resources used are either already
              preserved somewhere or provided along with the new dataset
              considered for preservation.</p>
            <aside class="example">
              <p>A dataset targetted for preservation is made of the following
                triples:</p>
              <pre style="" class="highlight prettyprint prettyprinted"> # Definition of a person
 ex:bob a ex:Staff;
     foaf:basedNear dbpedia:Cardiff;
     foaf:knows ex:john.
                 </pre>
              <p>Those triples make use of the "foaf" vocabulary and a custom
                one defined in the testing domain name "ex". It also uses
                entities defined in "dbpedia" and "ex". FOAF is a well
                established ontology that is archived in several places on the
                web (see, for instance, <a href="http://lov.okfn.org">the LOV
                  repository</a>). Entities defined in DBpedia are also
                preserved through their <a href="http://mementoweb.org/depot/native/dbpedia/">Memento
                  gateway</a> and archived dumps of the dataset also exists. The
                risks associated to preserving the triple making use of those
                external resource is thus minimal. A bigger concerns rises from
                the usage made of resources defined in "ex" which is a name
                space that, by design, does not exist outside of the dataset.
                Unless the data describing "ex:john" and "ex:Staff" is preserved
                alongside those triples their contextual meaning will be lost.
                This is in particular critical for "ex:Staff" as without it the
                type of "ex:bob" is unknown.</p>
              <p>Considering this assessement, a revised dataset including the
                definition of "ex:Staff" can be considered for preservation:</p>
              <pre style="" class="highlight prettyprint prettyprinted"> # Definition of a person
 ex:bob a ex:Staff;
     foaf:basedNear dbpedia:Cardiff;
     foaf:knows ex:john.
 # Custom vocabulary element
 ex:Staff rdfs:subClass foaf:Person;
     rdfs:label "A staff member". 
                 </pre>
              <p>This second, more complete, dataset is better suited for
                preservation as it is more self-describing and make use of
                external entities whose preservation is trusted.</p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>Datasets making references to portions of the Web of Data which
              are not preserved should receive a lower score than those using
              common resources.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>:<a href="http://www.w3.org/TR/dwbp-ucr/#R-VocabReference">R-VocabReference</a></p>
          </section>
        </div>
        <!-- end of assess dataset BP -->
        <!-- begin of serialisation BP -->
        <div class="practice">
          <p><span id="Serialisation" class="practicelab">Use a trusted
              serialisation format for preserved data dumps</span></p>
          <p class="practicedesc">Data depositors willing to send a datadump for
            long term preservation must use a well established serialisation</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Web data is an abtract data model that can be expressed in
              different ways (RDF/XML, JSON-LD, ...). Using a well established
              serialisation of this data increases its chances of re-use. </p>
            <p>Institute doing digital preservation are tasked with monitoring
              file format obsolescence. Datasets which have been acquired in
              some format some years ago may have to be converted into another
              format in order to still be usable with more modern software (see
              [[ROSENTHAL]]). This tasks can be made more challenge, or even
              impossible, if non standard serialisation formats are used by data
              depositors.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>It should be possible to read and load the dataset into a
              database even its software is no longer supported.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>Give preference to Web data serialisation formats available as
              open standards. For instance those provided by the W3C
              [[FORMATS]]. </p>
            <aside class="example">
              <p>Those triples are serialised using the Turtle W3C
                recommendation. It is a text-based format which is supported by
                the majority of software able to process Web data. This format
                can thus be trusted for preservation.</p>
              <pre style="" class="highlight"> # Definition of a person
 ex:bob a ex:Staff;
     foaf:basedNear dbpedia:Cardiff;
     foaf:knows ex:john.
                 </pre>
              <p>A custom-made serialisation of the same data such as what
                follows should be given a negative appreciation towards
                preserving the dataset.</p>
              <pre style="" class="highlight">ex:bob,a,ex:staff;###,foaf:basedNear,dbpedia:Cardiff;###,foaf:knows,ex:john.
                 </pre>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <!--<p>Try to open the data dump with different
            software.</p> -->
            <p>Try to dereference the URI of the data dump with Content-Type
              header according to the format you expect to get, using for
              example [[cURL]]</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>:<a href="http://www.w3.org/TR/dwbp-ucr/#R-FormatStandardized">R-FormatStandardized</a></p>
          </section>
        </div>
        <!-- end of serialisation BP -->
        <!-- begin of resource status BP -->
        <div class="practice">
          <p><span id="ResourceStatus" class="practicelab">Update the status of
              identifiers</span></p>
          <p class="practicedesc">Preserved resources should be linked with
            their "live" counterparts</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>URI dereferencing is a primary interface to data on the Web.
              Linking preserved datasets with the original URI inform the data
              consumer of the status of these resources.</p>
            <p>During its life cycle a dataset may undergo several
              modifications. Although URIs assigned to things are not expected
              to change, the description of these resource will evolve over
              time.
              <!--There are also some new IRIs that will be put into use, some
              other that will become deprecated, and some that will get deleted. -->
              During this evolution, several snapshots could be made available
              for preservation and access as versions.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>A link is maintained between the URI of a resource, the most
              up-to-date description available for it, and preserved
              descriptions. If the resource does not exist any more the
              description should say so and refer to the last preserved
              description that was available.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>There is a variety of HTTP status codes that could be put into
              use to relate the URI with its preserved description. In
              particular, 200, 410 and 303 can be used for different scenarios:</p>
            <ul>
              <li>200 =&gt; there is a new description which contains pointers
                to archived description</li>
              <!--              <li>404 =&gt; the resource just died, the URI consumer has to go
                find an archive to look for it</li> -->
              <li>410 =&gt; the resource is no longer available but it has been
                removed under a controlled process cf. 404 which simply states
                that something is not available.</li>
              <li>303 =&gt; the resource identified by this URI is no longer
                served here but there is a preserved description at a different
                location.</li>
              <!--              <li>209 =&gt; this resource does not exist any more but we have
                some information about it. The description could include a list                of locations having different preserved descriptions over                different times.</li> -->
            </ul>
            <p>In addition to the status codes, HTTP Link headers can also be
              used to relate resources to preserved descriptions.</p>
            <aside class="example">
              <p>One approach with link header is to use the Memento protocol to
                give a link to a timegate providing access to preserved
                descriptions of the resource:</p>
              <pre class="highlight"> 
curl -I http://example.org/dataset-001

HTTP/1.1 200 OK
Memento-Datetime: Sun, 05 April 2015 00:00:00 GMT
Link: http://example.org/dataset; rel=‚Äúoriginal‚Äù, http://example.org/timegate/dataset; rel=‚Äútimegate‚Äù
            </pre>
              <p>Using HTTP status code the data consumer can be redirected to
                the most recent description of the entity. In the following
                example a request for the resource
                "http://example.org/dataset-001" is first redirected to the
                description "http://example.org/data/dataset-001" which, as it
                has been preserved and flagged as invalid, redirects the client
                to the newer description
                "http://example.org/newdata/dataset-001"</p>
              <pre class="highlight">curl -L -I http://example.org/dataset-001

HTTP/1.1 303 See Other
Location: http://example.org/data/dataset-001
Link: http://example.org/newdata/dataset-001, rel="new"

HTTP/1.1 303 See Other
Location: http://example.org/newdata/dataset-001
Link: http://example.org/data/dataset-001, rel="previous"
           
HTTP/1.1 200 Ok
           </pre>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <p>Check that de-referencing the URI of a preserved dataset returns
              information about its current status and availability.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>:<a href="http://www.w3.org/TR/dwbp-ucr/#R-AccessLevel">R-AccessLevel</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-PersistentIdentification">
                R-PersistentIdentification</a></p>
          </section>
        </div>
        <!-- end of resource status BP --> </section>
      <!-- end of Data Preservation -->
      <!-- begin Feedback -->
      <section id="feedback">
        <h3>Feedback</h3>
        <p>Publishing data on the Web enables data sharing on a large scale,
          providing data access to a wide range of audiences with different
          levels of expertise. Data publishers want to ensure that the data
          published is meeting the data consumer needs and user feedback is
          crucial. Feedback has benefits for both data publishers and data
          consumers, helping data publishers to improve the integrity of their
          published data, as well as to encourage the publication of new data.
          Feedback allows data consumers to have a voice describing usage
          experiences (e.g. applications using data), preferences and needs.
          When possible, feedback should also be publicly available for other
          data consumers to examine. Making feedback publicly available allows
          users to become aware of other data consumers, supports a
          collaborative environment, and allows user community experiences,
          concerns or questions are currently being addressed.</p>
        <p>From a user interface perspective there are different ways to gather
          feedback from data consumers, including site registration, contact
          forms, quality ratings selection, surveys and comment boxes for
          blogging. From a machine perspective the data publisher can also
          record metrics on data usage or information about specific
          applications consumers are currently relying upon. Feedback such as
          this establishes a line of communication channel between data
          publishers and data consumers. In order to quantify and analyze usage
          feedback, it should be recorded in a machine-readable format. Blogs
          and other publicly available feedback should be displayed in a
          human-readable form through the user interface. </p>
        <p> This section provides some BP to be followed by data publishers in
          order to enable data consumers to provide feedback about the consumed
          data. This feedback can be for humans or machines. </p>
        <!-- begin of BP Gather Feedback -->
        <div class="practice">
          <p><span id="GatherFeedback" class="practicelab">Gather feedback from
              data consumers </span></p>
          <p class="practicedesc"> Data publishers should provide a means for
            consumers to offer feedback.</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Providing feedback contributes to improving the quality of
              published data, may encourage publication of new data, helps data
              publishers understand data consumers needs better and, when
              feedback is made publicly available, enhances the consumers'
              collaborative experience.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>It should be possible for data consumers to provide feedback and
              rate data in both human and machine-readable formats. The feedback
              should be Web accessible and it should provide a URL reference to
              the corresponding dataset.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>Provide data consumers with one or more feedback mechanisms
              including, but not limited to: a registration form, contact form,
              point and click data quality rating buttons, or a comment box for
              blogging.</p>
            <p>Collect feedback in machine-readable formats to represent the
              feedback and use a vocabulary to capture the semantics of the
              feedback information.
              <!-- The definition of a Data Usage Vocabulary is
              included in the activity of the DWBP group in order to support in              the implementation of this best practice. --></p>
            <aside class="example">
              <p>to be done </p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <ul>
              <li>Demonstrate how feedback can be collected from data consumers.
              </li>
              <li>Verify that the feedback is persistently stored. If the
                feedback is made publicly available verify that a URL links back
                to the published data being referenced.</li>
              <li>Check that the feedback format conforms to a known
                machine-readable format specification in current use among
                anticipated data users. </li>
            </ul>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-UsageFeedback">R-UsageFeedback</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-QualityOpinions">R-QualityOpinions</a></p>
          </section>
        </div>
        <!-- end of BP Gather Feedback -->
        <!-- begin of BP Information about Feedback -->
        <div class="practice">
          <p><span id="FeedbackInformation" class="practicelab"> Provide
              information about feedback </span></p>
          <p class="practicedesc"> Information about feedback should be
            provided.</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>Sharing information about feedback allows data consumers to be
              aware of feedback given by other consumers. </p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>It should be possible for humans to have access to information
              that describes feedback on a dataset given by one or more data
              consumers. </p>
            <p>It should be possible for machines to automatically process
              feedback information about a dataset.</p>
          </section>
          <section class="how">
            <p class="subhead">Possible Approach to Implementation</p>
            <p>The machine readable version of the feedback metadata may be
              provided according to the vocabulary that is being developed by
              the DWBP working group , i.e., the Dataset Usage Vocabulary
              [[DUV]].</p>
            <!-- The definition of a Data Usage Vocabulary is
              included in the activity of the DWBP group in order to support in              the implementation of this best practice. -->
            <aside class="example">
              <p>to be done </p>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to Test</p>
            <ul>
              <li>Check that the metadata for the dataset itself includes
                feedback information about the dataset.</li>
              <li>Check if a computer application can automatically process
                feedback information about the dataset. </li>
            </ul>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements</span>: <a href="http://www.w3.org/TR/dwbp-ucr/#R-UsageFeedback">R-UsageFeedback</a>,
              <a href="http://www.w3.org/TR/dwbp-ucr/#R-QualityOpinions">R-QualityOpinions</a></p>
          </section>
        </div>
        <!-- end of BP Gather Feedback --> </section>
      <!-- end Feedback -->
      <!-- begin enrichment -->
      <section id="enrichment">
        <h3>Data Enrichment</h3>
        <div class="issue">
          <p> To discuss about enrichment yields derived data, not just
            metadata. For example, you could take a dataset of scheduled and
            real bus arrival times and enrich it by adding on-time arrival
            percentages. The percentages are data, not metadata. <a href="http://www.w3.org/2013/dwbp/track/issues/196">Issue-196</a></p>
        </div>
        <div class="issue">
          <p> To discuss about the meaning of the word ‚Äútopification‚Äù. <a href="http://www.w3.org/2013/dwbp/track/issues/196">Issue-196</a></p>
        </div>
        <p>Data enrichment refers to a set of processes that can be used to
          enhance, refine or otherwise improve raw or previously processed data.
          This idea and other similar concepts contribute to making data a
          valuable asset for almost any modern business or enterprise. It also
          shows the common imperative of proactively using this data in various
          ways.</p>
        <p>This section provides some advice to be followed by data publishers
          in order to enable data consumers to enrich data.</p>
        <!-- begin of BP Data Enrichment -->
        <div class="practice">
          <p><span id="EnrichData" class="practicelab">Enrich data by generating
              new metadata.</span></p>
          <p class="practicedesc">Data should be enriched whenever possible,
            generating richer metadata to represent and describe it.</p>
          <section class="axioms">
            <p class="subhead">Why</p>
            <p>There is a large number of intelligent techniques that can be
              used to enrich raw or previously treated data and to extract new
              metadata from it, making data an even more valuable asset. These
              methods include those focused on data categorization, entity
              recognition, sentiment analysis, topification, among others.
              Providing new and richer metadata may help data consumers to
              better understand the data they are dealing with.</p>
          </section>
          <section class="outcome">
            <p class="subhead">Intended Outcome</p>
            <p>Describe a dataset using richer sets of metadata, which can be
              readable by humans.</p>
          </section>
          <section class="how">
            <p class="subhead"> Possible Approach to Implementation</p>
            <p> The implementation depends on what types of metadata should be
              produced. They require the implementation of methods for data
              categorization, disambiguation, sentiment analysis, among others,
              according to the suggestions described in <a href="enrichment.html">Data
                Enrichment Technical Note</a>. After new metadata is extracted,
              it can be provided as part of an HTML Web page or any open data
              format.</p>
            <aside class="example">
              <pre>to be done</pre>
            </aside>
          </section>
          <section class="test">
            <p class="subhead">How to test</p>
            <p>Check whether the metadata being extracted by the techniques are
              in accordance with human-knowledge and can be readable by humans.</p>
          </section>
          <section class="ucr">
            <p class="subhead">Evidence</p>
            <p><span>Relevant requirements:</span> <a href="http://www.w3.org/TR/dwbp-ucr/#R-DataEnrichment">R-DataEnrichment</a></p>
          </section>
        </div>
        <!-- end of BP Data Enrichment --> </section>
      <!-- end Enrichment -->
      <!-- end best practices --> </section>
    <section id="conclusions">
      <h2>Conclusions</h2>
    </section>
    <section id="glossary">
      <h2>Glossary</h2>
      <dl>
        <dt><dfn id="dataset">Dataset</dfn></dt>
        <dd>
          <p>A dataset is defined as a collection of data, published or curated
            by a single agent, and available for access or download in one or
            more formats. A dataset does not have to be available as a
            downloadable file.</p>
          <p>From: <a href="http://www.w3.org/TR/vocab-dcat/">Data Catalog
              Vocabulary (DCAT)</a></p>
        </dd>
        <dt><dfn id="citation">Citation</dfn></dt>
        <dd>
          <p>A Citation may be either direct and explicit (as in the reference
            list of a journal article), indirect (e.g. a citation to a more
            recent paper by the same research group on the same topic), or
            implicit (e.g. as in artistic quotations or parodies, or in cases of
            plagiarism).</p>
          <p>From: <a href="http://www.essepuntato.it/lode/http://purl.org/spar/cito">CiTO</a></p>
        </dd>
        <dt><dfn id="data_consumer">Data consumer</dfn></dt>
        <dd>
          <p>For the purposes of this WG, a Data Consumer is a person or group
            accessing, using, and potentially performing post-processing steps
            on data.</p>
          <p>From: Strong, Diane M., Yang W. Lee, and Richard Y. Wang. "Data
            quality in context." Communications of the ACM 40.5 (1997): 103-110.
          </p>
        </dd>
        <dt><dfn id="data_format">Data format</dfn></dt>
        <dd>
          <p>Data Format defined as a specific convention for data
            representation i.e. the way that information is encoded and stored
            for use in a computer system, possibly constrained by a formal data
            type or set of standards."</p>
          <p>From: <a href="http://guide.dhcuration.org/representation/">DH
              Curation Guide</a></p>
        </dd>
        <dt><dfn id="data_producer">Data producer</dfn></dt>
        <dd>
          <p>Data Producer is a person or group responsible for generating and
            maintaining data.</p>
          <p>From: Strong, Diane M., Yang W. Lee, and Richard Y. Wang. "Data
            quality in context." Communications of the ACM 40.5 (1997): 103-110.
          </p>
        </dd>
        <dt><dfn id="data_representation">Data representation</dfn></dt>
        <dd>
          <p>Data representation is any convention for the arrangement of
            symbols in such a way as to enable information to be encoded by a
            data producer and later decoded by data consumers."&gt;Data
            representation</p>
          <p>From: <a href="http://guide.dhcuration.org/representation/">DH
              Curation Guide</a></p>
        </dd>
        <dt><dfn id="feedback">Feedback</dfn></dt>
        <dd>
          <p>Feedback is a forum used to collect messages posted by consumers
            about a particular topic. Messages can include replies to other
            consumers. Datetime stamps are associated with each message and the
            messages can be associated with a person or submitted anonymously.</p>
          <p><a href="http://rdfs.org/sioc/spec/#sec-modules-types">SIOC</a>,
            (2) <a href="http://www.w3.org/TR/annotation-model/#motivations">Annotation#Motivation</a></p>
          <p>To better understand why annotation (See Annotation) was created <a

              href="http://www.w3.org/TR/skos-reference/">SKOS</a> is used to
            show inter-related annotation between communities with more
            meaningful distinctions than a simple class/subclass tree.</p>
        </dd>
        <dt><dfn id="data_preservation">Data preservation</dfn></dt>
        <dd>
          <p> Data Preservation is defined by <a href="http://www.alliancepermanentaccess.org/index.php/consultancy/dpglossary/#Preservation">APA</a>
            as "The processes and operations in ensuring the technical and
            intellectual survival of objects through time". This is part of a
            data management plan <a href="http://guide.dhcuration.org/preservation/">focusing
              on preservation planning and meta-data</a>. Whether it is
            worthwhile to put effort into preservation depends on the (future)
            value of the data, the resources available and the opinion of the
            stakeholders (= designated community). </p>
        </dd>
        <dt><dfn id="data_archiving">Data archiving</dfn></dt>
        <dd>
          <p>Data Archiving is the set of practices around the storage and
            monitoring of the state of digital material over the years. </p>
          <p>These tasks are the responsibility of a Trusted Digital Repository
            (TDR), also sometimes referred to as <a href="http://tools.ietf.org/html/rfc4810">Long-Term
              Archive Service (LTA)</a>. Often such services follow the <a href="http://en.wikipedia.org/wiki/Open_Archival_Information_System">Open
              Archival Information System</a> which defines the archival process
            in terms of ingest, monitoring and re-use of data.</p>
        </dd>
        <dt><dfn id="data_provenance">Data Provenance </dfn></dt>
        <dd>
          <p> Provenance originates from the French term "provenir" (to come
            from), which is used to describe the curation process of artwork as
            art is passed from owner to owner. Data provenance, in a similar
            way, is metadata that allows data providers to pass details about
            the data history to data users.</p>
        </dd>
        <dt><dfn id="data_quality">Data Quality </dfn></dt>
        <dd>
          <p> Data quality is commonly defined as ‚Äúfitness for use‚Äù for a
            specific application or use case.</p>
        </dd>
        <dt><dfn id="file_format">File format</dfn></dt>
        <dd>
          <p> File Format is a standard way that information is encoded for
            storage in a computer file. It specifies how bits are used to encode
            information in a digital storage medium. File formats may be either
            proprietary or free and may be either unpublished or open.</p>
          <p>Examples of file formats: <a href="https://en.wikipedia.org/wiki/Text_file#.TXT">txt</a>,
            <a href="https://en.wikipedia.org/wiki/Portable_Document_Format">pdf</a>,
            <a href="https://en.wikipedia.org/wiki/Postscript">ps</a>,<a href="https://en.wikipedia.org/wiki/Audio_Video_Interleave">avi</a>,
            <a href="https://en.wikipedia.org/wiki/GIF">gif</a> or <a href="https://en.wikipedia.org/wiki/JPEG">jpg</a>
          </p>
        </dd>
        <dt><dfn id="license">License </dfn></dt>
        <dd>
          <p> A license is a legal document giving official permission to do
            something with the data with which it is associated.</p>
          <p>From: <a href="http://dublincore.org/documents/2010/10/11/dcmi-terms/">
              DC-TERMS</a></p>
        </dd>
        <dt><dfn id="locale_parameter">Locale </dfn></dt>
        <dd>
          <p> A locale is a set of parameters that defines specific data
            aspects, such as language and formatting used for numeric values and
            dates.</p>
        </dd>
        <dt><dfn id="machine_readable">Machine Readable Data</dfn></dt>
        <dd>
          <p> Machine Readable Data are data formats that may be readily parsed
            by computer programs without access to proprietary libraries. For
            example <a href="https://en.wikipedia.org/wiki/Comma-separated_values">CSV</a>
            and <a href="http://www.w3.org/TR/2014/NOTE-rdf11-primer-20140624/#section-graph-syntax">RDF
              turtle family for graphs</a> are machine readable, but <a href="http://www.data.gov/developers/blog/primer-machine-readability-online-documents-and-data">PDF</a>
            and <a href="https://en.wikipedia.org/wiki/JPEG">JPEG</a> are not.
          </p>
          <p> From: <a href="http://www.w3.org/TR/ld-glossary/#vocabulary">Linked
              Data Glossary</a> </p>
        </dd>
        <dt><dfn id="sensitive_data">Sensitive Data </dfn></dt>
        <dd>
          <p> Sensitive data is any designated data or metadata that is used in
            limited ways and/or intended for limited audiences. Sensitive data
            may include personal data, corporate or government data, and
            mishandling of published sensitive data may lead to damages to
            individuals or organizations.</p>
        </dd>
        <dt><dfn id="vocabulary">Vocabulary</dfn></dt>
        <dd>
          <p> Vocabulary is A collection of "terms" for a particular purpose.
            Vocabularies can range from simple such as the widely used <a href="">RDF
              Schema</a>, <a href="http://xmlns.com/foaf/spec/">Foaf</a> and <a

              href="https://en.wikipedia.org/wiki/Dublin_Core#Dublin_Core_Metadata_Element_Set_Version_1.1">Dublin
              Core Metadata Element Set</a> to complex vocabularies with
            thousands of terms, such as those used in healthcare to describe
            symptoms, diseases and treatments. Vocabularies play a very
            important role in Linked Data, specifically to help with data
            integration. The use of this term overlaps with Ontology.</p>
          <p> From: <a href="http://www.w3.org/TR/ld-glossary/#vocabulary">Linked
              Data Glossary</a> </p>
        </dd>
        <dt><dfn id="structured_data">Structured data</dfn></dt>
        <dd>
          <p> Structured Data refers to data that conforms to a fixed schema.
            Relational databases and spreadsheets are examples of structured
            data.</p>
        </dd>
      </dl>
    </section>
    <section id="requirements">
      <h2>Use Cases Requirements x Best Practices</h2>
      <table id="uripatternstable">
        <caption>Requirements x Best Practices</caption>
        <tbody>
          <tr>
            <th>UC Requirement</th>
            <th>Best Practice</th>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-AccessBulk">R-AccessBulk</a></td>
            <td> Best Practice 19: Provide bulk download, Best Practice 20:
              Follow REST principles when designing APIs</td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-AccessLevel">R-AccessLevel</a></td>
            <td> Best Practice 18: Provide data unavailability reference, Best
              Practice 26: Update the status of identifier</td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-AccessRealTime">R-AccessRealTime</a></td>
            <td> Best Practice 21: Provide real-time access</td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-AccessUpToDate">R-AccessUpToDate</a></td>
            <td> Best Practice 22: Provide data up to date</td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-APIDocumented">R-APIDocumented</a></td>
            <td> Best Practice 20: Follow REST principles when designing APIs</td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-Citable">R-Citable</a></td>
            <td> Best Practice 10: Use persistent URIs as identifiers, Best
              Practice 11: Assign URIs to dataset versions and series</td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-DataEnrichment">R-DataEnrichment</a></td>
            <td> Best Practice 29: Enrich data by generating new metadata</td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-DataIrreproducibility">R-DataIrreproducibility</a></td>
            <td> <br>
            </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-DataLifecyclePrivacy">R-DataLifecyclePrivacy</a></td>
            <td> <br>
            </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-DataLifecycleStage">R-DataLifecycleStage</a></td>
            <td> <br>
            </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-DataMissingIncomplete">R-DataMissingIncomplete</a></td>
            <td> <br>
            </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-DataVersion">R-DataVersion</a></td>
            <td> Best Practice 8: Provide versioning information, Best Practice
              9: Provide version history, Best Practice 23: Maintain separate
              versions for a data API</td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-FormatLocalize">R-FormatLocalize</a></td>
            <td> Best Practice 3: Provide locale parameters metadat, Best
              Practice 9: Provide version history, Best Practice 23: Maintain
              separate versions for a data API</td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-FormatMachineRead">R-FormatMachineRead</a></td>
            <td> Best Practice 12: Use machine-readable standardized data
              formats </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-FormatMultiple">R-FormatMultiple</a></td>
            <td> Best Practice 13: Provide data in multiple formats </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-FormatStandardized">R-FormatStandardized</a></td>
            <td> Best Practice 12: Use machine-readable standardized data
              formats, Best Practice 25: Use a trusted serialisation format for
              preserved data dumps</td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-FormatOpen">R-FormatOpen</a></td>
            <td> Best Practice 12: Use machine-readable standardized data
              formats </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-GeographicalContext">R-GeographicalContext</a></td>
            <td> <br>
            </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-GranularityLevels">R-GranularityLevels</a></td>
            <td> <br>
            </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataAvailables">R-MetadataAvailable</a></td>
            <td> Best Practice 1: Provide metadata, Best Practice 2: Provide
              descriptive metadata, Best Practice 3: Provide locale parameters
              metadata, Best Practice 4: Provide structural metadata, Best
              Practice 6: Provide data provenance information </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataDocum">R-MetadataDocum</a></td>
            <td> Best Practice 1: Provide metadata </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataMachineRead">R-MetadataMachineRead</a></td>
            <td> Best Practice 1: Provide metadata, Best Practice 2: Provide
              descriptive metadata, Best Practice 5: Provide data license
              information </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-MetadataStandardized">R-MetadataStandardized</a></td>
            <td> Best Practice 2: Provide descriptive metadata, Best Practice 5:
              Provide data license information, Best Practice 14: Use
              standardized terms </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-PersistentIdentification">R-PersistentIdentification</a></td>
            <td> Best Practice 26: Update the status of identifiers </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-QualityComparable">R-QualityComparable</a></td>
            <td> Best Practice 16: Choose the right formalization level </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-QualityMetrics">R-QualityMetrics</a></td>
            <td> <br>
            </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-QualityOpinions">R-QualityOpinions</a></td>
            <td> Best Practice 27: Gather feedback from data consumers, Best
              Practice 28: Provide information about feedback </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-TrackDataUsage">R-TrackDataUsages</a></td>
            <td> <br>
            </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-UsageFeedback">R-UsageFeedback</a></td>
            <td> Best Practice 27: Gather feedback from data consumers, Best
              Practice 28: Provide information about feedback </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-VocabDocum">R-VocabDocum</a></td>
            <td> Best Practice 16: Choose the right formalization level </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-VocabOpen">R-VocabOpen</a></td>
            <td> <br>
            </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-VocabReference">R-VocabReference</a></td>
            <td> Best Practice 15: Re-use vocabularies, Best Practice 16: Choose
              the right formalization level </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-VocabVersion">R-VocabVersion</a></td>
            <td> Best Practice 24: Assess dataset coverage </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-UniqueIdentifier">R-UniqueIdentifier</a></td>
            <td> Best Practice 10: Use persistent URIs as identifiers, Best
              Practice 11: Assign URIs to dataset versions and series </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-LicenseAvailable">R-LicenseAvailable</a></td>
            <td> Best Practice 5: Provide data license information </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-LicenseLiability">R-LicenseLiability</a></td>
            <td> <br>
            </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-ProvAvailable">R-ProvAvailable</a></td>
            <td> Best Practice 6: Provide data provenance information </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-SensitivePrivacy">R-SensitivePrivacy</a></td>
            <td> Best Practice 17: Preserve people's right to privacy </td>
          </tr>
          <tr>
            <td> <a href="http://www.w3.org/TR/dwbp-ucr/#R-SensitiveSecurity">R-SensitiveSecurity</a></td>
            <td> Best Practice 17: Preserve people's right to privacy </td>
          </tr>
        </tbody>
      </table>
    </section>
    <h2>Acknowledgements</h2>
    <p>The editors gratefully acknowledge the contributions made to this
      document by all members of the working group and the chairs: Hadley
      Beeman, Steve Adler, Yaso C√≥rdova, Deirdre Lee.</p>
  </body>
</html>
